{
	"nodes":[
		{"id":"e5a49d94c9d758c9","type":"text","text":"### Линейная регрессия\n$X: (n, p)$, $w: (p, 1)$, $Y: (n, 1)$ \n$$ Xw = \\hat{Y} \\approx Y $$\nРешение (дифференцируем MSE по $w$):\n$$ \\frac{\\partial L} {\\partial w} = 2X^T(Xw - y) $$\n$$ w^* = (X^T X)^{-1}X^TY $$\n\n**Теорема Гаусса-Маркова**:\nЛинейная регрессия: $Y = f(X) + \\varepsilon$\n1) $E(\\varepsilon_i) = 0$\n2) $\\textrm{Var}(\\varepsilon_i) = \\sigma_i^2 < \\textrm{inf}$\n3) $\\textrm{Cov}(\\varepsilon_i, \\varepsilon_j) = 0$\nТогда минимизация MSE лосса даёт Best Linear Unbiased Estimation (BLUE)\n\\*Best: estimator with minimal variance from all unbiased estimators \n$E(w^*) = w_{\\textrm{true}}$\n$\\textrm{Var}(w^*)$ - $\\textrm{min}$\n\nВозможна нестабильность: $\\textrm{det} (X^T X) = 0$\n**Регуляризация**: \n1) L2 / Ridge\n$$L_2 = ||Y - Xw||_2^2 + \\lambda^2 ||w||_2^2$$\n$$w^* = (X^T X + \\lambda^2 I)^{-1}X^T Y$$\n2) L1 / Lasso\n$$L_1 = ||Y - Xw||_2^2 + \\lambda^2 ||w||_1$$\n3) Elastic net\n$$L_{EN} = ||Y - Xw||_2^2 + \\lambda_1^2 ||w||_1 + \\lambda_2^2 ||w||_2^2 $$\n\n**Обучение:** вычисляем $w$ по формуле либо используем градиентную оптимизацию\n$$ w \\leftarrow w + \\textrm{lr} \\cdot \\frac{\\partial L} {\\partial w} $$","x":-1160,"y":387,"width":500,"height":1013,"color":"4"},
		{"id":"76867bac8e65caec","type":"text","text":"### Наивный байесовский классификатор\n\nПредположение: фичи независимы\n$$P(y_i = C_k | x_i) = \\frac{P(x_i|y_i = C_k) P(y_i = C_k)}{P(x_i)} = \\frac{P(x_i|y_i = C_k) P(y_i = C_k)}{\\sum_i P(x_i |y_i = C_k) P(C_k)}\n$$\n\n$$P(x_i |y_i = C_k) = \\prod_{l=1}^p P(x_i^l | y_i = C_k)$$\n$P(y_i = C_k)$ из частотной оценки, $P(x_i^l | y_i = C_k)$ из параметрического распределения (e.g. параметры - выборочные среднее, дисперсия по каждому признаку по $x_i$ таким, что $y_i =C_k$)\n$$ C^*(x_i) = \\textrm{argmax}_k P(y_i = C_k | x_i)$$","x":-2300,"y":388,"width":560,"height":412,"color":"4"},
		{"id":"2960bbdde63356d1","type":"text","text":"### kNN\n\n**Гиперпараметры**: $k$ - число ближайших соседей, 'weights' - то, как взвешиваются соседи в зависимости от расстояния\n**Обучение**: просто запоминаем обучающую выборку\n**Инференс** (классификация): считаем расстояние от объекта инференса до всех объектов трейна, находим $k$ ближайших из них, вектор вероятности находится так:\n$$P(y = C_k | x) = \\frac{\\sum_{i=1}^k w(x_i) [y_i = C_k \\ \\& \\ x_i - \\textrm{сосед} \\ x]}{\\sum_{i=1}^k w(x_i) [x_i - \\textrm{сосед} \\ x]}$$\n**Инференс** (регрессия): предсказываем взвешенное среднее по соседям","x":-1720,"y":387,"width":500,"height":413,"color":"4"},
		{"id":"ca91d745cdc31a93","type":"text","text":"## Линейные модели*\n\nклассы моделей, которые предполагают линейную зависимость между объектом и таргетом \n\n\\*неформально\n","x":-980,"y":60,"width":360,"height":260,"color":"4"},
		{"id":"a81434c50c8051ee","type":"text","text":"### Semi-supervised learning\n- Нужно разметить данные\n- NLP (предсказание токена)","x":-2642,"y":-1735,"width":342,"height":175,"color":"4"},
		{"id":"105c2e42bd83e762","type":"text","text":"## Supervised learning\nДатасет: $\\{(x_i, y_i)\\}_{i = 1}^n$\nРегрессия: $x_i \\in \\mathbb{R^p}, y_i \\in \\mathbb{R}$\nКлассификация: $x_i \\in \\mathbb{R^p}, y_i \\in \\{ 1, 2, ... , N \\}$\nМодель $f(x)$\nЛосс $L(x, y, f)$\nЭмпирический риск $Q(x,y,f)$ - средний лосс на трейн объектах:\n$$Q(X) = \\sum_{i=1}^n L(y^i, f(x^i))$$","x":-3753,"y":-1730,"width":415,"height":320,"color":"4"},
		{"id":"39d4cb1d068d2d07","type":"text","text":"## Unsupervised learning\nСнижение размерности, кластеризация","x":-3145,"y":-1730,"width":366,"height":210,"color":"4"},
		{"id":"4900ae031b1d8613","type":"text","text":"### Регрессия\n\n**Лоссы**:\nMSE (квадрат L2-нормы): \n$$||Y - \\hat{Y}||_2^2 = (Y -\\hat{Y})^T (Y -\\hat{Y})$$\nMAE (L1-норма):\n$$||Y - \\hat{Y}||_1 = \\frac{1}{n}\\sum_i |y_i - \\hat{y}_i| $$\n\n**Метрики**:\n1) MSE\n2) MAE\n3) RMSE: $\\sqrt{\\textrm{MSE}}$\n4) MAPE - Mean Absolute Percentage Error\n$$ \\textrm{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y - \\hat{y}_i|}{|y_i|}$$\n5) SMAPE - Symmetric MAPE:\n$$ \\textrm{SMAPE} = \\frac{1}{n} \\sum_{i=1}^n \\frac{2 \\cdot |y_i - \\hat{y}_i|}{|y_i| + |\\hat{y}_i|}$$\n7) R2: \n$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y}_i)^2}$$\n\n","x":-4066,"y":-1240,"width":520,"height":730,"color":"4"},
		{"id":"44e9d43972a24777","type":"text","text":"### Классификация\n\n**Лоссы**:\n- $Q(M) = (1 - M)^2$\n- $V(M) = (1 - M)_+$\n- $S(M) = 2(1 + e^M)^{-1}$\n- $L(M) = \\textrm{log}_2(1 + e^{-M})$\n- $E(M) = e^{-M}$\n- $\\textrm{Cross-entropy loss} = -\\sum_{i} y_i \\textrm{log}(p_i)$\n\n**Метрики**:\n1) Accuracy: \n$$ \\textrm{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n [y_i = \\hat{y}_i] $$\n2) Balanced accuracy:\n$$ \\textrm{Balanced accuracy} = \\frac{1}{C} \\sum_{k=1}^C \\frac{\\sum_i [y_i = k \\ \\& \\ y_i = \\hat{y}_i]}{\\sum_i [y_i = k]}$$\n3) Precision, Recall\n$(\\hat{y}, y)$: $(1,1)$ - $TP$, $(0, 0)$ - $TN$, $(1, 0)$ - $FP$, $(0, 1)$ - $FN$  \n$$\\textrm{Precision} = \\frac{TP}{TP + FP} = \\frac{\\#(1, 1)}{\\#(1, \\_)}$$\n$$\\textrm{Recall} = \\frac{TP}{TP + FN} = \\frac{\\#(1, 1)}{\\#(\\_, 1)}$$\n4) F-score\n$$F_1 = \\frac{2}{\\textrm{precision}^{-1} + \\textrm{recall}^{-1}}$$\n$$F_\\beta = (1 + \\beta^2) \\frac{\\textrm{precision} \\cdot \\textrm{recall}}{\\beta^2\\textrm{precision} + \\textrm{recall}}$$\n\n5) ROC, ROC-AUC\nСортируем объекты по принадлежности к выбранному классу, двигаем границу, по которой принимаем решение отнесения к данному классу. **Сортируем по возрастанию => идём слева направо, объекты слева от бордера к негативному классу, кривая строится справа налево.** Строим на основании этого график в осях $\\textrm{FPR}$, $\\textrm{TPR}$\n$$\\textrm{FPR} = \\frac{FP}{FP + TN} = \\frac{\\#(1, 0)}{\\#(\\_, 0)}$$\n$$\\textrm{TPR} = \\frac{TP}{TP + FN} = \\frac{\\#(1, 1)}{\\#(\\_, 1)}$$\n\n- Baseline - random predictions\n- Always above diagonal (for reasonable classifier)\n- Strictly higher curve $\\Rightarrow$ better classifier\n\nПояснение: \n- Бордер равен 0 $\\Rightarrow \\textrm{FPR} = 0, \\ \\textrm{TPR} = 0$. Все объекты относим к негативному классу, числители обнуляются\n- Когда бордер переходит через вероятность каждого объекта, график делает скачок\n- бордер равен 1 $\\Rightarrow \\textrm{FPR} = 0, \\ \\textrm{TPR} = 0$\n\n\n 1) Precision-Recall Curve\n\n**Многоклассовые метрики:**\n3) \"samples\" (по объектам):\n$$ \\frac{1}{|S|} \\sum_{s \\in S} F(y_s, \\hat{y}_s)$$\n\n4) \"macro\" (по классам):\n$$ \\frac{1}{|L|} \\sum_{l \\in L} F(y_l, \\hat{y}_l)$$\n5) \"weighted\" (согласно мощностям классов:\n$$ \\frac{1}{\\sum_{l \\in L}|\\hat{y}_l|} \\sum_{l \\in L} |\\hat{y}_l| F(y_l, \\hat{y}_l)$$\n\n**Confusion matrix:**\nМатрица (predicted label, true label)","x":-3400,"y":-1240,"width":583,"height":2040,"color":"4"},
		{"id":"9aeba1ae97928e9b","type":"file","file":"Screenshot_20250109_170651.png","x":-2780,"y":-360,"width":319,"height":280},
		{"id":"2a09e8b49ca3bca3","type":"file","file":"Screenshot_20250109_172646.png","x":-2780,"y":232,"width":400,"height":288},
		{"id":"178ae237204e163d","type":"text","text":"## Метрические модели\n\n$$a(x, X^l) = \\textrm{argmax}_{y \\in Y} \\sum_{i=1}^l [y^{(i)} = y] w(i,x)$$\n","x":-1580,"y":60,"width":360,"height":130,"color":"5"},
		{"id":"f3094a960959e10b","type":"text","text":"### Метод потенциальных функций\n\n$$w(i, x) = \\gamma^{(i)} K \\left( \\frac{\\rho(x, x^{(i)})}{h^{(i)}} \\right)$$","x":-1714,"y":824,"width":344,"height":186,"color":"5"},
		{"id":"d990b18bf79a096b","type":"text","text":"### Метод Надарая-Ватсона","x":-1540,"y":1050,"width":340,"height":60,"color":"5"},
		{"id":"0b4a0a74fde338ce","type":"text","text":"$$ \\alpha^* = (F^TF)^{-1}F^T y= F^+ y$$\n$FF^+$ - проекционная матрица. $P_F y$ - проекция\nМНК - проецирование $y$ на линейную оболочку столбцов матрицы $F$. Размерность оболочки $\\leq p$, размерность пространства $n$\n\n**SVD**: $$F = VDU^T$$\n- $V^T V = I_n$, столбцы $v_j$ - собственные векторы $FF^T$\n- $U^T U = I_n$, столбцы $u_j$ - собственные векторы $F^T F$\n- $D = \\textrm{diag}(\\sqrt{\\lambda}_1, ..., \\sqrt{\\lambda}_n)$, $\\lambda_j \\geq 0$ - собственные значения $F^T F$, $FF^T$ \nSVD in LR:\n$$||F\\alpha - y||^2 \\rightarrow \\textrm{min}_\\alpha$$\n$$F^+ = (F^T F)^{-1} F^T = \\sum_{j=1}^n \\frac{1}{\\sqrt{\\lambda_j}} u_j v_j^T$$\n$$\\alpha^* = F^+ y = UD^{-1}V^t y = \\sum_{i=1}^n \\frac{1}{\\sqrt{\\lambda_j}} u_j (v_j^Ty)  $$\n$$F\\alpha^* = P_F y = \\sum_{j=1}^n v_j (v_j^T y)$$\n$$ ||\\alpha^*||^2 = \\sum_{j=1}^n \\frac{1}{\\lambda_j} (v_j^T y)^2 $$\nЧисло обусловленности:\n$$ \\mu(S) = \\frac{\\lambda_{max}}{\\lambda_{min}}$$\nОно является коэффициентом усиления относительных погрешностей\n$F^T F$ плохо обусловлена $\\Rightarrow$ решение неустойчиво, плохо интерпретируемо, $||\\alpha||^2$ велико, возникает переобучение. Исправляется регуляризацией, отбором признаков, преобразование признаков. \n- Ridge Regression: $\\lambda_j \\mapsto \\lambda_j + \\tau$\n- LASSO: $\\sum_{j=1}^n |\\alpha_j|$. $\\alpha_j = \\alpha_j^+ - \\alpha_j^-$,  $\\alpha_j^+ \\geq 0$, $\\alpha_j^- \\geq 0$ $\\Rightarrow$ чем больше коэффициент регуляризации, тем больше таких $j$, что $\\alpha_j^+ = \\alpha_j^- = 0$\n- Elastic Net, Support Features Machine\n","x":-1210,"y":1440,"width":600,"height":1040,"color":"4"},
		{"id":"711c6d8dea128331","type":"text","text":"# Общее","x":-3175,"y":-2480,"width":358,"height":120},
		{"id":"ab7c296aa1dce17c","type":"text","text":"## Валидация\n\n**Hold-out**\n**Кросс-валидация**\n- LOO\n- KFold\n- StratifiedKFold - поддерживаем баланс классов (условно если в трейне 80/20, то и в валидации должно быть так же) \n- GroupKFold - данные содержат группы, и мы не хотим, чтобы одна и та же группа попадала одновременно в трейн и валидацию. Пример: есть данные нескольких пациентов, не хотим, чтобы данные одного пациента попали одновременно в трейн и валидацию. Тогда обучаемся на одной части пациентов, а валидируемся на остальных.","x":-2227,"y":-2606,"width":467,"height":486,"color":"4"},
		{"id":"d211a661d972a59b","type":"text","text":"## Принцип максимального правдоподобия\n$$ L(\\theta |X, Y) = P(X, Y | \\theta) \\rightarrow \\textrm{max}$$\ni.i.d. $\\Rightarrow$ $L(\\theta |X, Y)  = \\prod_{i} P(x_i, y_i|\\theta)$\nequivalent to \n$$\\textrm{log}L(\\theta|X,Y) = \\sum_{i} P(X_i, y_i | \\theta) \\rightarrow \\textrm{max}_\\theta$$","x":-2227,"y":-1909,"width":507,"height":260,"color":"4"},
		{"id":"67b052bf574e1269","type":"text","text":"# Statistics\n","x":-660,"y":-2600,"width":500,"height":157},
		{"id":"5485010d3f73138e","type":"text","text":"### p-value\n**Def.** Probability under the null hypothesis of obtaining a real-valued test statistic at least as extreme as the one obtained. Test-statistic $t$, unknown distribution $T$.\n$$ p = 2 \\textrm{min} \\{ \\textrm{Pr}(T \\geq t | H_0), \\textrm{Pr}(T \\leq t | H_0) \\}$$**Usage**:\nOne choose a model (the null hypothesis) and the alpha level $\\alpha \\ (0.05)$. p-value  $\\leq \\alpha$ $\\Rightarrow$ reject null hypothesis\n\n**E.g.** Tossing a coin. Hypothesis: even if I got $N$ heads in a row with $N$ tosses, my coin is no different from a normal coin ($p = 0.5$).\nEquiv hypothesis: in a coin with $B(p)$, $p = p_0 := 0.5$.\n$p_0^N$. p-val: $2 \\cdot p_0^N$. p-val $\\leq 0.05 \\Leftrightarrow$ $p_0 \\leq 0.025^{1/N}$\n\n**p-hacking**","x":-1380,"y":-2320,"width":500,"height":460},
		{"id":"ad118c1fcaa3e756","type":"text","text":"## False discovery rate\n**False positive**: samples from the same distribution, but p-value shows that from the separate distributions.\nRare: 95% of the times samples will overlap, 5% don't\n\nExample. a) 10k samples from the same distribution $\\Rightarrow$ about $5\\%$ p-values < 0.05. b) 10k samples from separate distributions $\\Rightarrow$ most of p-values > 0.05, others are **false negatives**\nAdjusting p-values","x":-833,"y":-2320,"width":553,"height":460},
		{"id":"7a98b719b456c5ec","type":"text","text":"## Power\nWhen there's a lot of overlap between 2 distributions, we have a small sample size, we have relatively low power\n\n**Power analysis** determines what sample size will ensure a high probability that will correctly reject the Null Hypothesis\nFactors: overlap, number of samples\n\nEx. Power = 0.8. Threshold for significance. $\\alpha = 0.5$. \n$$\\textrm{Effect Size(d)} = \\frac{\\textrm{Estimated diff in the means}}{\\textrm{Pooled estimated standart deviations}}$$\n$$\\sqrt{\\frac{s_1^2 + s_2^2}{2}}$$","x":-226,"y":-2312,"width":526,"height":372},
		{"id":"dae4baa9a75b0b7d","type":"text","text":"$X \\times Y$ - в.п. с $p(x, y | w) = P(y|x,w) p(x)$ \nОМП:\n$$ \\prod_{i=1}^l p(x_i, y_i|w) = \\prod_{i=1}^l P(y_i |x_i, w)p(x_i) \\rightarrow \\textrm{max}_w$$\n\nМаксимизация log-likelihood:\n$$ L(w) = \\sum_{i=1}^l \\textrm{log}P(y_i | x_i, w) \\rightarrow \\textrm{max}_w$$\nМинимизация эмпирического риска:\n$$Q(w) = \\sum_{i=1}^l\\mathcal{L}(y_i g(x_i, w)) \\rightarrow \\textrm{min}_w$$\nЭквивалентность:\n$$ - \\textrm{log}P(y_i | x_i, w) = \\mathcal{L}(y_i g(x_i, w))$$\n**Вероятностный смысл регуляризации**\n$$L(w) = \\textrm{ln}p(X^l, w) = \\sum_{i=1}^l \\textrm{log} P(y_i | x_i, w) + \\textrm{log} p (w; \\gamma) \\rightarrow \\textrm{max}_w$$ L1: $$p(w, C) = \\frac{1}{(2 \\pi C)^{n/2}} \\textrm{exp} (- \\frac{||w||^2}{2C})$$\nL2: $$ \\frac{1}{(2C)^n} \\textrm{exp}(-\\frac{||w||}{C})$$","x":-2227,"y":-1600,"width":507,"height":740,"color":"3"},
		{"id":"2dd1c99d0464d69c","type":"text","text":"## Деревья","x":440,"y":60,"width":383,"height":82,"color":"4"},
		{"id":"9f1942470c005519","type":"text","text":"### Decision trees\n\n$$G(j,t) = \\frac{|L|}{|H|} H(L) + \\frac{|R|}{|Q|}H(R) \\rightarrow \\textrm{min}_{j,t}$$\n Gini impurity:\n$$G = 1 - \\sum_k (p_k)^2$$\nEntropy criteria:\n$$H(R) = - \\sum_k p_k \\textrm{log}_2 p_k$$\nRegression:\n$$ H(R) = \\textrm{min}_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\nMissing values:\n$\\hat{y} = w_L \\hat{y}_L + w_R \\hat{y}_R$\n\nDT as Linear Models:\n$$ \\hat{y} = \\sum_j w_j [x \\in J_j] $$\nCategorical features\n\n**CART**\nMinimal Cost-Complexity Pruning\n\n$$ C_\\alpha(a) = \\sum_{i=1}^l (a(x_i) - y_i)^2 + \\alpha |V_{лист}| \\rightarrow \\textrm{min}_a$$","x":80,"y":383,"width":640,"height":777,"color":"4"},
		{"id":"86865be2b6fcab10","type":"text","text":"### SVM - метод опорных векторов\n$$Q(w, w_0) = \\sum_{i=1}^l [M_i(w,w_0) < 0] \\leq \\sum_{i=1}^l (1 - M_i(w,w_0))_+ + \\frac{1}{2C}||w||^2 \\rightarrow \\textrm{min}_{w,w_0}$$\n**Геометрия:**\n1) Выборка линейно разделима $\\Rightarrow \\exists w, w_0: \\forall i ( M_i(w, w_0) > 0)$\nНормировка: $\\textrm{min}_i M_i(w, w_0) = 1$. Строим разделяющую полосу.\n$\\exists x_+: \\langle w, x_+ \\rangle - w_0 = +1$, $\\exists x_-: \\langle w, x_- \\rangle - w_0 = -1$ \nШирина полосы максимальна:\n$$\\frac{\\langle x_+ - x_-, w \\rangle}{||w||} = \\frac{2}{||w||} \\rightarrow \\textrm{max}$$\n2) Ослабление условий:\n$$ \\begin{cases} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^l \\xi_i \\rightarrow \\textrm{min}_{w,w_0, \\xi} \\\\\n\\xi_i \\ge 1 - M_i(w,w_0), i = 1..l \\\\\n\\xi_i \\ge 0, i = 1..l\n\\end{cases}$$\nСистема эквивалентна изначальной постановке\n\n**Теорема Каруша-Куна-Таккера**\nПостановка двойственной задачи заависит только от скалярного произведения\n$\\langle x, x_i \\rangle \\rightarrow K(x,x_i)$. $K$ - скалярное произведение в каком-то пространстве\n\nЯдра:\n- $\\langle x, x' \\rangle ^ d$\n- $(\\langle x, x' \\rangle + 1) ^ d$\n- $\\textrm{th}(k_1\\langle x, x' \\rangle - k_0)$\n- $\\textrm{exp}(-\\gamma||x - x'||^2)$\n\nРегрессия:\n$$ \\sum_{i=1}^l (|\\langle w, x_i \\rangle - w_0 - y_i| - \\delta)_+ + \\frac{1}{2C}||w||^2 \\rightarrow \\textrm{min}_{w, w_0}$$","x":-560,"y":1240,"width":700,"height":880,"color":"3"},
		{"id":"f0fa4dfd056cabc5","type":"text","text":"### Random forest\n\nBootstrap: $N$ datasets from $m$ objects with replacement\n$\\varepsilon_j(x) = b_j(x) - y(x)$. $E_x(b_j(x)- y(x))^2 = E_x\\varepsilon_j^2(x)$\nMean error of N models:\n$$E_1 = \\frac{1}{N} \\sum_{j=1}^N E_x \\varepsilon_j^2(x)$$\nConsider the errors unbiased and uncorrelated (lie but we can use the idea anyway):\n$E_x \\varepsilon_j(x) =0$, $E_x \\varepsilon_i(x) \\varepsilon_j(x) = 0, i \\neq g$\n$a(x) = \\frac{1}{N} \\sum_{j=1}^N b_j(x)$\n$$E_N = E_x(\\frac{1}{N} \\sum_{j=1}^n b_j(x) - y(x))^2 = \\frac{1}{N} E_1$$\nBagging - bootstrap aggregating. Many trees that are complex enough\n\n**Random forest**: Bagging + RSM (random subspace methods)\nRSM - for every split choose random subspace of features\n- universal\n- better with missing values\n- Extremely Randomized Trees, Isolation Forest\n- OOB - out of bag\n\n**We can train in parallel on a forest level!**","x":823,"y":383,"width":640,"height":777,"color":"4"},
		{"id":"4a556e0a837837d9","type":"text","text":"### Ансамблирование","x":1360,"y":60,"width":376,"height":80},
		{"id":"188b32d526ffdfba","type":"text","text":"### Бустинг, градиентный бустинг\n\n**Интуиция:** строим модели последовательно, новые исправляют результат предыдущего ансамбля\n\n$$ \\hat{f}_T(x) = \\sum_{t=1}^T \\rho_t h_t(x)$$\n**Adaboost** $E(M) = e^{-M}$:\n$$L(y_i, \\hat{f}(x_i)) = \\textrm{exp}(-y_i \\hat{f}_T(x_i)) = \\textrm{exp}(-y_i \\sum_{t=1}^{T-1}\\rho_t h_t(x_i)) \\cdot \\textrm{exp} (-y_i \\rho_T h_T (x_i))$$\n**Gradient boosting**:\n$$\\hat{f}(x) = \\textrm{argmin}_{f(x)} L(y, f(x)) = \\textrm{argmin}_{f(x)} \\mathbb{E}_{x,y}[L(y, f(x))]$$\n$$\\hat{f}(x) = f(x, \\hat{\\theta}), \\ \\hat{\\theta} = \\textrm{argmin}_\\theta \\mathbb{E}_{x,y} [L(y, f(x,\\theta))]$$\n\nОграничимся ансамблями из других моделей\n $$\\hat{f}(x) = \\sum_{t=1}^{t-1} \\hat{f}_i(x)$$\n $$(\\rho_t, \\theta_t) = \\textrm{argmin}_{\\rho, \\theta} \\mathbb{E}_{x,y}[L(y, \\hat{f}(x) + \\rho h (x, \\theta))],  \\hat{f}_t(x) = \\rho_t h(x, \\theta_t)$$\n \"Аппроксимируем антиградиент на каждой точке\"\n $$r_{it} = - \\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x) = \\hat{f}(x)}$$\n $$\\theta_t = \\textrm{argmin}_\\theta \\sum_{i=1}^n (r_{it} - h(x_i, \\theta))^2$$\n $$\\rho_t = \\textrm{argmin}_\\rho \\sum_{i=1}^n (L(y_i, \\hat{f}(x_i) + \\rho(x_i, \\theta_t))$$\nНужно брать не слишком сложные модели (переобучение, не будет градиентов)\n \n\n \n\n ","x":1548,"y":383,"width":652,"height":1097,"color":"3"},
		{"id":"62ad461c35fc77c1","type":"text","text":"### Blending\nВзвешенное голосование\n$$f = \\sum_{i=1}^n \\rho_i f_i$$На одной подвыборке обучаем модель, на другой подбираем коэффициенты\n\nPros:\n- simple and intuitive\n- can average several blendings\nCons:\n- Linear composition is not always enough\n- Need to split data","x":2280,"y":383,"width":360,"height":549},
		{"id":"18613406d0b13213","type":"text","text":"### Stacking\n\n1. Разбиваем данные на фолды\n2. Обучаем на группе фолдов, предсказываем на оставшемся\n3. Обучаем новую модель на мета-фичах (конкатенированных предсказаниях)\nМожно применять рекурсивно","x":2720,"y":383,"width":440,"height":549},
		{"id":"405d340b78d024c5","type":"text","text":"## Какие-то лекции","x":-119,"y":-3111,"width":359,"height":71},
		{"id":"4bc79fb180ae4ba7","type":"text","text":"### 1.1\n\n**Генеральная совокупность**\n**Выборка**\n**Репрезентативная** выборка\n**Статистика** - функция от выборки\n**Меры центральной тенденции и меры разброса**\n**Несмещённая выборочная дисперсия:**\n$$ \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{x})^2$$\n**Перцентиль** порядка $k$ - такое число, что $k\\%$ выборки меньше этого числа\n### 1.2\n**Эмпирическая функция распределения**\n\n$$ F(x) = P(X \\leq x)$$ $$ \\hat{F}_n(x) = \\hat{P} (X \\leq x) = \\frac{1}{n} \\sum_{i=1}^n [X_i \\leq x]$$\n**Ящик с усами*","x":400,"y":-3640,"width":542,"height":720},
		{"id":"0a304bea0f81ec8a","type":"text","text":"### 2.1\n$$Cov(X,Y) = E[(X-EX)(Y-EY)] = E(XY) - EXEY$$\n$$\\textrm{corr}(X) = \\frac{\\textrm{Cov}(X,Y)}{\\sqrt{DX DY}}$$\n### 2.2\n$X = (X_1, X_2) \\sim N((\\mu_1, \\mu_2), \\textrm{Cov}(X)) = N (\\mu, \\Sigma)$\n\n### 2.3\nЯдерные оценки плотности\n$$ \\hat{f}_n(x) = \\frac{1}{nh} \\cdot \\sum K(\\frac{x - x_i}{h})$$\n$K(z) \\geq 0$, $\\int K(z) dz = 1$. Ex. $\\mathcal{N}(0,1)$\n### 2.4\nБорьба с пропусками и выбросами\n### 2.5\nЛогарифмирование","x":1040,"y":-3626,"width":542,"height":692},
		{"id":"f6b0d2cf7d9bf72d","type":"text","text":"### 3.1\n**Распределение хи-квадрат**\n$$ Y = X_1^2 + ... + X_k^2 \\sim \\chi_k^2$$$$ f(x) = \\frac{1}{2^{k/2} \\Gamma (\\frac{k}{2})} x^{k/2 - 1} e^{-x/2}, x \\geq 0$$\n$E(X) = k, D(X) = 2k$\n**Распределение Стьюдента**\n$X_0 \\sim N(0,1), Y \\sim \\chi_k^2$\n$$Z = \\frac{X_0}{\\sqrt{Y/k}} \\sim t(k)$$\n$$\\frac{\\bar{x}}{\\sqrt{\\hat{\\sigma^2/n}}}$$\n$EZ = 0, DZ = k / (k+2)$\n\n**Распределение Фишера**\n$X \\sim \\chi_k^2, Y \\sim \\chi_m^2$. \n$$ Z = \\frac{\\sqrt{X/k}}{\\sqrt{Y/m}}$$","x":1687,"y":-3626,"width":553,"height":692},
		{"id":"3d4fad686ecf3f5a","type":"text","text":"### 3.2\n**ЗБЧ**\n$X_1, ..., X_n$ i.i.d, $DX_i < \\infty$\n$$\\bar{X} = \\frac{X_1 + ... + X_n}{n} \\xrightarrow{p} E(X_1)$$\n### 3.3\n**Сходимость по вероятности**\n$$\\forall \\varepsilon > 0 P(|X_n - X| < \\varepsilon) \\rightarrow 1$$\n\n### 3.4\n**ЦПТ**\n$X_1, ..., X_n$ попарно независимые, одинаково распределённые, $DX_i < \\infty$\n$$\\frac{X_1 + ... + X_n}{n} \\xrightarrow{d} N(E(X_1), D(X_1)/n)$$\n$$\\sqrt{n} \\frac{\\bar{X}_n - EX_1}{\\sqrt{DX_1}} \\xrightarrow{d} N(0,1)$$\nСумма достаточно большого числа iid случайных величин имеет близкое к нормальному распределению\n\n**Сходимость по распределению:**\nпоследовательность функций распределения сходится к данной\n$$\\textrm{lim}_{n \\rightarrow \\infty} F_{X_n}(x) = F_X(x) $$\n\n**ЗБЧ vs. ЦПТ**\nЗБЧ: одно среднее, посчитанное по выборке размера $n$. При росте $n$ среднее стабилизируется\nЦПТ: много средних, посчитанных по разным выборкам размера $n$. При росте $n$ распределение всё больше похоже на нормальное\n### 3.5\nСходимость почти наверное\n$$P(\\textrm{lim}_{n \\rightarrow \\infty} X_n = X) = 1$$\n\n**Сильный ЗБЧ**\nУсловия слабого + $E(|X_1|) < \\infty$ $\\Rightarrow$ сходимость п.н. ","x":2340,"y":-3626,"width":460,"height":1346},
		{"id":"6578286bcf2a7408","type":"text","text":"## Кластеризация\n\n- Средее внутриклассовое расстояние -> min\n- Среднее межклассовое расстояние -> max","x":3640,"y":80,"width":480,"height":177,"color":"5"},
		{"id":"16250eceb049658d","type":"text","text":"### K-means\nМинимизация внутрикластерных расстояний\n$$\\sum_{i=1}^l ||x_i - \\mu_{a_i}||^2 \\rightarrow \\textrm{min}_{a_i, \\mu_a}$$\nВход: выборка, число классов, начальное приближение. Выход центры кластеров\n\n**k-means++**","x":3240,"y":383,"width":480,"height":275,"color":"5"},
		{"id":"0d56449cf722e91d","type":"text","text":"### DBSCAN\n- корневой - с плотной окрестностью\n- граничный - не корневой, но в окрестности\n- шумовой (выброс) - не корневой и не граничный","x":3755,"y":383,"width":445,"height":275,"color":"5"},
		{"id":"6e39c95b75181299","type":"text","text":"### EM-алгоритм","x":3240,"y":720,"width":480,"height":53,"color":"5"},
		{"id":"2f02e91a4ad7f577","type":"text","text":"### Иерархическая кластеризация","x":4240,"y":383,"width":440,"height":65,"color":"5"},
		{"id":"8ad815160b3eb4a7","type":"text","text":"# Модели\n\n- Параметры, гиперпараметры\n- Under-fitting, appropriate-fitting, over-fitting","x":240,"y":-1600,"width":560,"height":200},
		{"id":"f08006a10ddd14b2","type":"text","text":"### Optimization\n- SGD\n$$w = w - lr \\cdot \\frac{\\partial L}{\\partial w}$$\nПроблема: локальные оптимумы, шум\n- SGD with momentum\n$$\\begin{cases} v_{t+1} = \\rho v_t + \\nabla f(x_t) \\\\ x_{t+1} = x_t - \\alpha v_{t+1}\\end{cases}$$\n- Nesterov momentum\n$$\\begin{cases} v_{t+1} = \\rho v_t - \\alpha f(x_t + \\rho v_t) \\\\ x_{t+1} = x_t + v_{t+1}\\end{cases}$$\n\n- Adagrad\n$$\\begin{cases} \\textrm{cache}_{t+1} = \\textrm{cache}_t + (\\nabla f(x_t))^2 \\\\ x_{t+1} = x_t - \\alpha \\frac{\\nabla f(x_t)}{\\textrm{cache}_{t+1} + \\varepsilon}\\end{cases}$$\n- RMSProp\n$$\\begin{cases} \\textrm{cache}_{t+1} = \\beta\\textrm{cache}_t + (1 - \\beta)(\\nabla f(x_t))^2 \\\\ x_{t+1} = x_t - \\alpha \\frac{\\nabla f(x_t)}{\\textrm{cache}_{t+1} + \\varepsilon}\\end{cases}$$\n- Adam (without bias correction)\n$$\\begin{cases} v_{t+1} = \\gamma v_t - (1 - \\gamma) \\nabla f(x_t) \\\\ \\textrm{cache}_{t+1} = \\beta\\textrm{cache}_t + (1 - \\beta)(\\nabla f(x_t))^2 \\\\ x_{t+1} = x_t - \\alpha \\frac{v_{t+1}}{\\textrm{cache}_{t+1} + \\varepsilon}\\end{cases}$$\n\n**Data normalization**\nBatchnorm:\n$$ y_i = \\gamma \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}} + \\beta$$\nDropout: $p \\Rightarrow$ turn off some neurons randomly on train","x":4320,"y":760,"width":505,"height":920,"color":"3"},
		{"id":"b418a2790c499589","type":"text","text":"### RNN\n\n**Vanilla RNN**\n$$\\begin{cases}h_t = \\textrm{tanh}(Wx_t + Uh_{t-1} + b) \\\\ o_t = c + Vh_t \\\\ \\hat{y}_t = \\textrm{softmax}(o_t)\\end{cases}$$Реализация может варьироваться. Где-то используется конкатенация инпута и скрытого состояния:\n$$\\begin{cases} h_{t} = \\sigma (W [h_{t-1}, x_t] + b) \\\\ P(x_{t}) = \\textrm{softmax}(Uh_{t-1} + c) \\end{cases}$$\nОбучение: \n- по выходному вектору\n- на сумму лоссов по моментам времени\n\n! Используем сигмоиду, т.к. каждый слой хочет получать на вход данные из одного и того же распределения. В случае неограниченных функций на вход скрытому слою может прийти инпут другого масштаба, помешает обучению\n! Batchnorm не используем, поскольку \"респонс на каждом временном шаге может иметь свои статистические зависимости\" (пример: предложения из 10 токенов с пунктуацией на конце - последний шаг заметно отличается от предыдущих)\n\nПроблемы:\n- Затухание градиентов: $$ \\frac{dh_t}{dh_{t-1}} \\frac{dh_{t-1}}{dh_{t-2}}$$\n- Боттлнек по вектору скрытого состояния, нет памяти\n\nМожно вставить классификатор в конец сети","x":4931,"y":760,"width":589,"height":920,"color":"4"},
		{"id":"71976a7b8f48894e","type":"text","text":"## NN\n**Activation functions**: ReLU, tanh, $\\sigma$, leaky (parametric) ReLU, GeLU\n- Problems of tanh and sigmoid: saturated grads\n- Problems of ReLU: not zero-centered output, zero grads when x < 0","x":5400,"y":129,"width":400,"height":257,"color":"4"},
		{"id":"de7ba7aabe75faca","type":"file","file":"RNN-vs-LSTM-vs-GRU-1024x308.png","x":5022,"y":1760,"width":997,"height":300},
		{"id":"a309f544df823db6","type":"text","text":"**Свёрточные сети**\n\nИзображение $32 \\times 32 \\times 3$, фильтр $5 \\times 5 \\times 3$ $\\rightarrow$ $1 \\times 28 \\times 28$\n$$\\textrm{out}_{N_i, C_{out_j}} = \\textrm{bias}(C_{out_j}) + \\sum_{k=0^{C_m-1}} \\textrm{weight}(C_{\\textrm{out}}, k) * \\textrm{input}(N_i, k)$$\n\nМожно (нужно!) несколько фильтров \nСвёрточный уровень: Conv + активация\nПример сети:\n((Conv, Relu) x 2, Pool) x3 -> FC\n\n- Фильтр $(f_1, f_2)$. Возвращает $(w - f_1 + 1, h - f_2 + 1)$\n\nТехники:\n- Паддинг $(p_1, p_2)$ (0, 'reflect', etc.) Увеличивает входной тензор на $(2p_1, 2p_2)$\n- Страйд $(s_1, s_2)$. Уменьшает аутпут в $(s_1, s_2)$ раз (в смысле целой части от деления; нужно следить за размерностями)\n- Пулинг $(k_1, k_2)$. Уменьшает аутпут в $(k_1, k_2)$ раз. Average, max\n\nLeNet-5: Conv, Pool\nAlexNet: Conv, Pool, ReLU, dropout, аугментации\nZFNet - примерно то же самое\nVGGNet - 138M параметров, кушает много памяти (93Mb/image)\nInception\nResNet\n\n- Residual connection","x":6240,"y":760,"width":640,"height":920,"color":"4"},
		{"id":"178ae058d3ce1957","type":"text","text":"### Логистическая регрессия\n$$c(x) = \\textrm{sgn}(f(x)) = \\textrm{sgn}(x^T w)$$\nMargin:\n$$ M_i = y_i \\cdot f(x_i) = y_i x_i^T w $$\n$$ L_{\\textrm{mis}} = [M_i \\leq 0]; L_{\\textrm{MSE}} = (y_i - x_i^Tw)^2(1 - M_i)^2$$ \nSigmoid:\n$p_+ = P(y = 1| x) \\in [0,1], \\textrm{log}\\frac{p_+}{1 - p_+} \\in \\mathbb{R}, y = x^Tw \\in \\mathbb{R}$\n$$p_+ = \\frac{1}{1 + \\textrm{exp}(-x^T w)} = \\sigma(x^T w)$$\nProperties:\n$$1 - \\sigma(x) = \\sigma(-x), \\ \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\nMLE:\n$$\\textrm{log}L(w|X,Y)  = \\sum_{i} P(x_i, y_i | w) = - \\sum_{i} \\textrm{log}(1 + \\textrm{exp}(-M_i)) \\rightarrow \\textrm{max}_w$$\n$y_i = 1: P(x_i, 1 | w) = \\sigma_w(x_i) = \\sigma_w(M_i)$\n$y_i = -1: P(x_i, -1|w) = \\sigma_w(-x_i) = \\sigma_w(M_i)$\n\nMulticlass:\n- One vs Rest (проблема - центроиды на одной прямой); $k$ классификаторов\n- One vs One $k(k-1)/2$ классификаторов","x":-560,"y":383,"width":580,"height":777,"color":"4"},
		{"id":"4992925d94d8a98d","type":"text","text":"**LSTM**\n$f_t$ - forget gate \n$i_t$ - input gate\n$o_t$ - output gate\n1) Бинарная классификация забыть старое/оставить\n$$f_t = \\sigma(W_f [h_{t-1}, x_t]) + b_f$$\n2) Бинарная классификация запомнить новое/не запоминать\n$\\tilde{C}_t$ - вектор с новой информацией\n$$\\begin{cases}i_t = \\sigma(W_i [h_{t-1}, x_t]) + b_i \\\\ \\tilde{C}_t = \\textrm{tanh} (W_C[h_{t-1}, x_t] + b_C\\end{cases}$$\n3) Применяем забывание/запоминание\n$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n4) Посчитали аутпут, новое скрытое состояние - смесь аутпута и памяти (смысл?)\n$$\\begin{cases}o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o) \\\\ h_t = o_t * \\textrm{tanh} (C_t) \\end{cases}$$Альтернативный вариант через сложения, без конкатенации\n\n**GRU**\n$r_t$ - reset gate\n$z_t$ - update gate\n$z_t$ говорит, какую часть $h_{t-1}$ нужно обновить, $(1-z_t)$ - какую часть оставить\n$r_t$ говорит, на какую часть $h_{t-1}$ опираться при подсчёте $\\tilde{h}_t$\n$$ \\begin{cases} z_t = \\sigma(W_z [h_{t-1}, x_t]) \\\\\nr_t = \\sigma(W_r [h_{t-1}, x_t]) \\\\ \\tilde{h}_t = \\textrm{tanh} (W [r_t * h_{t-1}, x_t]) \\\\ h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\\end{cases}$$","x":5600,"y":759,"width":588,"height":923,"color":"3"},
		{"id":"9f8ddbe4b65e81e9","type":"text","text":"## Bias-Variance tradeoff\n- The [_bias_](https://en.wikipedia.org/wiki/Bias_of_an_estimator \"Bias of an estimator\") error is an error from erroneous assumptions in the learning [algorithm](https://en.wikipedia.org/wiki/Algorithm \"Algorithm\"). High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n- The _[variance](https://en.wikipedia.org/wiki/Variance \"Variance\")_ is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random [noise](https://en.wikipedia.org/wiki/Noise_(signal_processing) \"Noise (signal processing)\") in the training data ([overfitting](https://en.wikipedia.org/wiki/Overfitting \"Overfitting\")).","x":-4200,"y":-2360,"width":536,"height":276,"color":"3"},
		{"id":"d0eb5e419a2a941f","type":"text","text":"## Bias-variance decomposition\n$X = (x_i, y_i)_{i=1}^l,  y_i \\in \\mathbb{R}$\n$L(a, y) = (a(x)-y)^2$\nМетод обучения: $\\mu: 2^X \\rightarrow A$\nЗадача минимизации среднеквадратичного риска:\n$$R(a) = E_{x,y}(a(x)- y)^2 = \\int_{X} \\int_{Y} (a(x)-y))^2 p(x,y) dxdy \\rightarrow \\textrm{min}_a$$\nИдеальный минимизатор среднеквадратичного риска:\n$$ a^*(x) = E(y | x) = \\int_Y y p(y|x)$$\nОсновная метрика качества метода обучения: \n$$ Q(\\mu) = E_{X^l}E_{x,y} (\\mu(X^l)(x) - y)^2$$\n**Теорема**:\nВ случае MSE для любого $\\mu$\n$$Q(\\mu) = E_{x,y}(a^*(x) - y)^2 + E_{x,y}(\\bar{a}(x) - a^*(x))^2 + E_{x,y}E_{X^l}(\\mu(X^l)(x) - \\bar{a}(x))^2,$$\nгде $\\bar{a}(x) = E_{X^l} (\\mu(X^l)(x))$","x":-5000,"y":-2360,"width":640,"height":735,"color":"3"},
		{"id":"dbca68f8c8e2d667","type":"text","text":"## PCA\n\n$f_1, ..., f_n$ - исходные числовые признаки\n$g_1, ..., g_m$ - новые числовые признаки ($m \\leq n$)\n\n**Требование**:\n$$\\hat{f}_j(x) = \\sum_{s=1}^m g_s(x) u_{js}, \\ \\sum_{i,j}(\\hat{f}_j(x_i) - f_j(x_i))^2 \\rightarrow \\textrm{min}_{g_s(x_i), u_{js}}$$\n$F, G, U; \\hat{F} = GU^T \\approx F$\n$$ ||GU^T - F||^2 \\rightarrow \\textrm{min}_{G,U}$$\n**Th**. $m \\leq \\textrm{rk} F$, то минимум $||GU^T - F||^2$ достигается, когда столбцы $U$ - с.в. $F^T F$, соответствующие $m$ максимальным с.з. $\\lambda_1, ... \\lambda_m$, а $G = FU$\nПри этом:\n- $U^T U = I_m$\n- $G^T G = \\Lambda = \\textrm{diag}(\\lambda_1, ..., \\lambda_m)$\n- $U \\Lambda = F^T F U; G \\Lambda = FF^T G;$\n- $||GU^T - F||^2 = ||F||^2  - \\textrm{tr} \\Lambda = \\sum_{j=m+1}^n \\lambda_j$ \n\nЕсли взять $m = n$, то:\n- $||GU^T - F||^2 = 0$;\n- $\\hat{F} = GU^T = F$ точное и совпадает с сингулярным разложением при $G = V \\sqrt{\\lambda}$:\n$$ F = GU^T = V\\sqrt{\\Lambda}U^T; \\ U^T U = I_m; \\ V^T V = I_m$$\n- $U$ работает в обе стороны:\n$$F = GU^T; \\ G = FU$$\n\n**Эффективная размерность выборки**\nУпорядочим с.з. $F^T F$ по убыванию: $\\lambda_1 \\geq ... \\geq \\lambda_n \\geq 0$\nЕсли есть \"крутой склон\", то берём с.в., отвечающие с.з. до склона\n","x":-1210,"y":2500,"width":600,"height":920},
		{"id":"7a7d65827a4ac0aa","type":"text","text":"**Кросс-энтропия через MLE**\n\nПредсказания модели: $P(\\hat{y}_i = C |  x_i, \\theta)$ для каждого класса $C \\in \\lbrace C_1, ..., C_k \\rbrace$. Т.е., для каждого $x$ параметрическая модель с $\\theta$ предсказывает вероятности $[p_1, ..., p_k]$. \n\n$$\\mathcal{L}(X, Y | \\theta) = \\prod_{i=1}^n P(y_i = \\hat{y}_i | x_i, \\theta) = \\prod_{i=1}^n\\prod_{C = 1}^k P(\\hat{y}_i = C |x_i, \\theta)^{\\delta_{y_i, C}}$$\n$$\\textrm{log} \\mathcal{L}(X, Y | \\theta) = \\sum_{i=1}^n \\sum_{C = 1}^k \\delta_{y_i, C} \\textrm{log} P(y_i = C | x_i, \\theta) = \\sum_{i=1}^n \\textrm{log} P(y_i = \\hat{y}_i | x_i, \\theta)$$\n$$H = - \\textrm{log}\\mathcal{L} \\rightarrow \\textrm{min}$$\n","x":-2263,"y":-800,"width":580,"height":320,"color":"3"}
	],
	"edges":[
		{"id":"c583c99f3037081b","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"ca91d745cdc31a93","toSide":"top"},
		{"id":"8e511b8e0be08f7b","fromNode":"ca91d745cdc31a93","fromSide":"bottom","toNode":"e5a49d94c9d758c9","toSide":"top"},
		{"id":"a03dbfdc949dfafd","fromNode":"ca91d745cdc31a93","fromSide":"bottom","toNode":"178ae058d3ce1957","toSide":"top"},
		{"id":"97ca480adb007213","fromNode":"ca91d745cdc31a93","fromSide":"bottom","toNode":"86865be2b6fcab10","toSide":"top"},
		{"id":"2be12b6e840b21fe","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"76867bac8e65caec","toSide":"top"},
		{"id":"cb4b75cbade335e5","fromNode":"711c6d8dea128331","fromSide":"bottom","toNode":"105c2e42bd83e762","toSide":"top"},
		{"id":"1b51ccfe9bf38e0a","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"178ae237204e163d","toSide":"top"},
		{"id":"9ca9ea87d4b5d1dd","fromNode":"178ae237204e163d","fromSide":"bottom","toNode":"2960bbdde63356d1","toSide":"top"},
		{"id":"30849295f5aef4be","fromNode":"178ae237204e163d","fromSide":"bottom","toNode":"d990b18bf79a096b","toSide":"top"},
		{"id":"87beddf694755f32","fromNode":"105c2e42bd83e762","fromSide":"bottom","toNode":"4900ae031b1d8613","toSide":"top"},
		{"id":"6be6d450a9e6ce0e","fromNode":"105c2e42bd83e762","fromSide":"bottom","toNode":"44e9d43972a24777","toSide":"top"},
		{"id":"bfbcc04977ff96e8","fromNode":"711c6d8dea128331","fromSide":"bottom","toNode":"39d4cb1d068d2d07","toSide":"top"},
		{"id":"fbe4bc6815375fcd","fromNode":"711c6d8dea128331","fromSide":"bottom","toNode":"a81434c50c8051ee","toSide":"top"},
		{"id":"4a7fbe097a8402db","fromNode":"711c6d8dea128331","fromSide":"right","toNode":"d211a661d972a59b","toSide":"left"},
		{"id":"9afa72c7fe31436c","fromNode":"178ae237204e163d","fromSide":"bottom","toNode":"f3094a960959e10b","toSide":"top"},
		{"id":"080886c9c2e1e491","fromNode":"e5a49d94c9d758c9","fromSide":"bottom","toNode":"0b4a0a74fde338ce","toSide":"top"},
		{"id":"bb3996b1d1773922","fromNode":"0b4a0a74fde338ce","fromSide":"bottom","toNode":"dbca68f8c8e2d667","toSide":"top"},
		{"id":"2e6893dc1578a655","fromNode":"711c6d8dea128331","fromSide":"right","toNode":"ab7c296aa1dce17c","toSide":"left"},
		{"id":"3fe93971983c23b4","fromNode":"67b052bf574e1269","fromSide":"bottom","toNode":"5485010d3f73138e","toSide":"top"},
		{"id":"aa0afbd3c60f456f","fromNode":"67b052bf574e1269","fromSide":"bottom","toNode":"ad118c1fcaa3e756","toSide":"top"},
		{"id":"101ead7735c328e3","fromNode":"67b052bf574e1269","fromSide":"bottom","toNode":"7a98b719b456c5ec","toSide":"top"},
		{"id":"22c9eb97e6bbc393","fromNode":"d211a661d972a59b","fromSide":"bottom","toNode":"dae4baa9a75b0b7d","toSide":"top"},
		{"id":"048cb1ef8010bb5e","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"2dd1c99d0464d69c","toSide":"top"},
		{"id":"df93eccb600c5263","fromNode":"2dd1c99d0464d69c","fromSide":"bottom","toNode":"9f1942470c005519","toSide":"top"},
		{"id":"ad2042d40a209510","fromNode":"2dd1c99d0464d69c","fromSide":"bottom","toNode":"f0fa4dfd056cabc5","toSide":"top"},
		{"id":"d42c8cba9c991c54","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"4a556e0a837837d9","toSide":"top"},
		{"id":"5a2671b4de6f2ec3","fromNode":"4a556e0a837837d9","fromSide":"bottom","toNode":"f0fa4dfd056cabc5","toSide":"top"},
		{"id":"f7d65524d60100b3","fromNode":"4a556e0a837837d9","fromSide":"bottom","toNode":"188b32d526ffdfba","toSide":"top"},
		{"id":"5de7693147ec0ab5","fromNode":"4a556e0a837837d9","fromSide":"right","toNode":"62ad461c35fc77c1","toSide":"top"},
		{"id":"6a42de06c9b0606c","fromNode":"4a556e0a837837d9","fromSide":"right","toNode":"18613406d0b13213","toSide":"left"},
		{"id":"38730dc1a4004812","fromNode":"67b052bf574e1269","fromSide":"right","toNode":"405d340b78d024c5","toSide":"left"},
		{"id":"e9387cf50c0a03de","fromNode":"405d340b78d024c5","fromSide":"right","toNode":"4bc79fb180ae4ba7","toSide":"left"},
		{"id":"dd4ac4addfcfe412","fromNode":"4bc79fb180ae4ba7","fromSide":"right","toNode":"0a304bea0f81ec8a","toSide":"left"},
		{"id":"9e09d73314015543","fromNode":"0a304bea0f81ec8a","fromSide":"right","toNode":"f6b0d2cf7d9bf72d","toSide":"left"},
		{"id":"d1d63046eaf29d15","fromNode":"f6b0d2cf7d9bf72d","fromSide":"right","toNode":"3d4fad686ecf3f5a","toSide":"left"},
		{"id":"deff361971e28f97","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"6578286bcf2a7408","toSide":"top"},
		{"id":"882238f9bc6b53ac","fromNode":"6578286bcf2a7408","fromSide":"bottom","toNode":"16250eceb049658d","toSide":"top"},
		{"id":"4bbc6208d7aebaff","fromNode":"16250eceb049658d","fromSide":"bottom","toNode":"6e39c95b75181299","toSide":"top"},
		{"id":"eb8cdd4039833a3a","fromNode":"6578286bcf2a7408","fromSide":"bottom","toNode":"0d56449cf722e91d","toSide":"top"},
		{"id":"4d94c9ae16156045","fromNode":"6578286bcf2a7408","fromSide":"bottom","toNode":"2f02e91a4ad7f577","toSide":"top"},
		{"id":"10bbbe49ec4447bc","fromNode":"8ad815160b3eb4a7","fromSide":"bottom","toNode":"71976a7b8f48894e","toSide":"top"},
		{"id":"b37c663ae8ac582b","fromNode":"71976a7b8f48894e","fromSide":"left","toNode":"f08006a10ddd14b2","toSide":"right"},
		{"id":"c0b3ab91812e545e","fromNode":"71976a7b8f48894e","fromSide":"bottom","toNode":"b418a2790c499589","toSide":"top"},
		{"id":"46416218d7de02ca","fromNode":"b418a2790c499589","fromSide":"right","toNode":"4992925d94d8a98d","toSide":"left","toEnd":"none"},
		{"id":"17863f28df83bd63","fromNode":"71976a7b8f48894e","fromSide":"bottom","toNode":"a309f544df823db6","toSide":"top"},
		{"id":"d6b4f2eb98f38e69","fromNode":"711c6d8dea128331","fromSide":"left","toNode":"9f8ddbe4b65e81e9","toSide":"right"},
		{"id":"f97f85bedb17af7d","fromNode":"9f8ddbe4b65e81e9","fromSide":"left","toNode":"d0eb5e419a2a941f","toSide":"right","toEnd":"none"},
		{"id":"c109f4808a0df14d","fromNode":"dae4baa9a75b0b7d","fromSide":"bottom","toNode":"7a7d65827a4ac0aa","toSide":"top"}
	]
}