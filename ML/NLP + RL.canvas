{
	"nodes":[
		{"id":"fef7d40ee4ba1073","type":"text","text":"### L1 Introduction\n\n1\\. **Bag of words** (e.g. \"the dog is on the table\" -> (0, 0, 1, 1, 0, 1, 1, 2, ...))\nProblems:\n- no info about words order\n- vectors are huge and sparse\n- not normalized\n- different forms of words\n\nCan be improved: n-grams -> collocations (meaningful n-grams)\n\n2\\. **Token normalization**\n- Stemming (removing suffixes)\n- Lemmatization (getting base form of the words)\n\\* WordNet; NLTK, BeautifulSoup, Regular expressions\n\nLeft: capital letters, punctuation, contractions, numbers, stop-words, tags\n\n3\\. **TF-IDF**\n$$\\textrm{tf}(\\textrm{term}, \\textrm{document}) = \\frac{\\# \\textrm{occurrences of term in document}}{\\# \\textrm{terms in document}}$$\n$$\\textrm{idf}(\\textrm{term}, \\textrm{Documents}) = \\log \\frac{\\# \\textrm{Documents}}{\\# \\textrm{Documents with term}}$$\n$$\\textrm{tf-idf} = \\textrm{tf} \\cdot \\textrm{idf}$$\nТерм в документе чаще => tf больше, терм встречается в меньшем количестве документов => idf больше\n\n4\\. **Matrix factorization**\nConsidering coocurrences in a window of fixed size\nPMI - pointwise mutual information\n$$\\textrm{PMI}(u,v) = \\log \\frac{p(u,v)}{p(u)p(v)} = \\log \\frac{n_{uv}}{n}\\cdot \\frac{n^2}{n_un_v}$$\n$$\\textrm{pPMI}(u,v) =\\max(0, \\textrm{PMI})$$\nSVD on $W$ with $W_{u,v} = PMI(u,v)$ => embeddings are rows of W\n\n5\\. **Word2Vec**\n\"... problems turning into banking crises as ...\" \n$P(w_{t-i}|w_t)$, $i = -\\textrm{window\\_size},...\\textrm{window\\_size}$\n\nIdea:\nFor pairs $(v, w)$ in the windows: $v$ -> one-hot, $w$ -> one-hot \nInput vector $v_{\\textrm{o-h}}$   ->    Hidden layer         ->    Output layer (SM) | answer is $w_{\\textrm{o-h}}$\n(10000)                        (300)                               (10000)            \n        (300, 10000)             (10000, 300)\nEmbeddings are columns of the first matrix or rows of the second\n\nSubsampling: subsampling frequent words to decrease the number of training examples\n$$P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.00}} + 1) \\cdot \\frac{0.001}{z(w_i)}$$\nNegative sampling\n\n! Skip-gram vs. continuous bag of words \n\n6\\. **GloVE** (Global Vectors for word representations)\n$$J = \\sum_{i,j} f(X_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij})$$","x":-420,"y":-200,"width":660,"height":1580},
		{"id":"e7ae13c73667b740","type":"text","text":"### L2 Seq2seq\n\n1\\. **Statistical MT**\n\n2\\. **NMT**\n$$P(y|x) = P(y_1|x)P(y_2 |y_1, x) P(y_3|y_1, y_2, x)...P(y_T|y_1, ..., x)$$\n**Seq2Seq**\nMachine learning is great -> SOS Машинное обучение прекрасно\n\nДекодинг: \n- teacher forcing\n- beam search - считаем наиболее вероятную цепочку \n$$\\textrm{score}(y_1, ..., y_t) = \\log P_{LM} (y_1, ..., y_t|x) = \\frac{1}{t}\\sum_{i=1}^T \\log P_{LM} (y_i|y_1, ..., y_{i-1},x)$$\n**BLEU**\n$$BLEU = \\textrm{brevity penalty} \\cdot (\\prod_{i=1}^n \\textrm{precision}_i)^{1/n} \\cdot 100\\%$$\n$$\\textrm{brevity pentalty} = \\min(1, \\frac{\\textrm{output length}}{\\textrm{reference length}})$$**ROUGE**, **METEOR**\n**BLEURT**, **XCOMET**, LLM-based\n\n3\\. **Attention**\n\nПример: сгенерировали слово \"я\" на основании скрытого состояния декодера $s_1$. Считаем скоры для $s_1$ и всех скрытых состояний\n\nПростейший случай: \n1) для скрытого состояния декодера $s_i$ и всех скрытых состояний энкодера $h_j$ считаем скоры: $$e_{ij} = \\textrm{score}(s_i, h_j) = s_i h_j$$\n2) нормализуем веса (softmax): $$\\alpha_{ij} = \\frac{\\exp (e_{ij})}{\\sum_{k=1}^T \\exp (e_{ik})}$$\n3) берём взвешенную сумму скрытых состояний: $$c_i = \\sum_{j=1}^T \\alpha_{ij}\\cdot h_j$$\n4) новое скрытое состояние декодера считается на основе $$[s_i, c_i]$$","x":240,"y":-200,"width":620,"height":1220},
		{"id":"f0e568870a569392","type":"text","text":"### L3 Transformer\n\n1\\. **Self-Attention**\n\nДля каждого из элементов последовательности создаём три новых вектора путём домножения на матрицы\n\n- Query (запрос) $Q$ - вектор, представляющий текущий элемент, для которого вычисляем attention. Матрица $W^Q$\n- Key (ключ) $K$ - векторы, с которыми сравнивается запрос. Матрица $W^K$\n- Values (значения) $V$ - векторы, из которых извлекается информация. Матрица $W^V$\n\n1) Упаковываем эмбеддинги из последовательности в матрицы, считаем матрицы $Q$, $K$, $V$: $$X \\cdot W^Q = Q, \\ X \\cdot W^K = K, \\ X \\cdot W^V = V$$\n2) Считаем скоры: $$\\textrm{Score}(Q_i, K_j) = \\langle Q_i, K_j \\rangle; \\ \\textrm{Scores} = QK^T$$\n3) Масштабирование (чтобы избежать проблем с градиентами при больших размерностях, $d_k$ - размерность ключа): $$\\textrm{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n4) Применяем Softmax, чтобы получать веса: $$\\textrm{Attention Weights} = \\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n5) Умножаем веса на векторы значений $V$: $$\\textrm{Output} = \\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n\n2\\. **Multi-Head Attention:** (аналогия с фильтрами в CNN)\n- несколько наборов $Q$, $K$, $V$ учатся фокусироваться на разных аспектах\n- результаты конкатенируются\nИмеем: $$X \\cdot W^Q_i = Q_i, \\ X \\cdot W^K_i = K_i, \\ X \\cdot W^V_i = V_i$$\n$$\\textrm{Output}_i = \\textrm{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right) V_i$$\n$$\\textrm{MultiHead}(Q,K,V) = \\textrm{Concat}(\\textrm{Output}_1, ..., \\textrm{Output}_h)W^O$$\n\n3\\. **Positional encoding**\nИзначально преобразования инвариантны к шаффлингу позиций\nПозиционные эмбеддинги захардкожены, добавляем их поэлементно \n$$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}}), \\ PE_{(cos, 2i + 1)} = \\sin(pos/10000^{2i/d_{model}})$$\nИдея: для достаточно больших длин последовательностей позиционные эмбеддинги не должны повторяться\n\n4\\. **Add & Normalize**\nAdd - skip connection ($F(X) + X$)\nNormalize - layer norm ((batch_size, seq_len, hid_size) -> (batch_size, seq_len))\nВ целом: к выходу self-attention добавляем вход, применяем layer norm, отправляем на feed forward, после FF ещё add+normalize\n\n5\\. **Decoder side**\nEncoder-decoder attention: masked self-attention (то же самое, что self-attention, но домножаем на маску)","x":860,"y":-200,"width":780,"height":1540},
		{"id":"f0ef840112b51e26","type":"text","text":"1\\. **Token normalization**: \n- Stemming - process that cuts off prefixes and suffixes\ne.g. running, runner, runs -> run / runn\n- Lemmatization - reducing words to their base/dictionary form (lemma)\ne.g. running -> run\nRemaining issues: capitals, punctiation, stopwords, etc. \n\n2\\. **Embeddings**:\n1) Sentence embeddings:\n\t-  Bag of words (n-grams, collocations - 2+ words that tend to appear frequently together)\n\t- TF-IDF\n2) Word embeddings (\\* definition of PMI, pPMI}):\n\t -   Matrix factorization\n\t -   Word2Vec\n\t -   GloVE","x":-420,"y":1380,"width":660,"height":500},
		{"id":"1608aa1ce424a80a","type":"text","text":"1\\. **Seq2Seq**\n- Encoder:  input sequence -> embeddings -> autoreg model ($h_t$, output$_t$, memory$_t$)\n- Decoder: (first token, hidden from encoder, optional: input seq (for attention), target seq (supervised learning))\n\n\\* RNN, LSTM, GRU\n\n2\\. **Metrics**: BLEU, ROUGE, METEOR; BLEURT, XCOMET; LLM-based\n\n**3**\\. **Basic attention**\nДля данного (основного?) скрытого состояния декодера \n1) считаем скалярные произведения со скрытыми состояниями энкодера\n2) берём от них софтмакс - это скоры\n3) складываем скрытые состояние энкодера с весами\n4) конкатенируем получившееся состояние с основным скрытым состоянием декодера","x":240,"y":1020,"width":620,"height":500},
		{"id":"9860029c98dc6aff","type":"text","text":"1\\. **Позиционный энкодинг**:\n- Классический: фиксированное синусоидальное кодирование (но GPT-3, GPT-4)\n- Современный\n1) обучаемые позиционные эмбеддинги (позиции 1, ..., max_len получают trainable векторы, добавляются к токенам)\n2) rotatory position encoding (вращение матриц, LLaMA)\n3) ALiBi (Attention with Linear Biases, 2022) - линейный штраф за дальние токены \n4) NoPE (локальное внимание + стратегии маршрутизации)\n$$X_{pos} = X + P[:\\textrm{seq\\_len}]$$  \n\n2\\. **Self-attention**\nЕсть упакованные в матрицу $X$: $(\\textrm{seq\\_len}, \\textrm{emb\\_dim})$ эмбеддинги входной последовательности, обучаемые матрицы $W_Q, \\ W_K, \\ W_V$: $(\\textrm{emb\\_dim}, d_k)$.\nИз них получаем $Q$, $K$, $V$. $Q$: $(\\textrm{seq\\_len}, d_k)$ - матрица запросов (аналогия: данное слово посылает запрос к остальным, чтобы оценить их релевантность), $K$: $(\\textrm{seq\\_len}, d_k)$ - матрица ключей (ключ сопоставляется с запросом). $QK^T$: $(\\textrm{seq\\_len}, \\textrm{seq\\_len})$ - матрица ответов на запросы. Далее делим на $\\sqrt{d_k}$ для стабильности, применяем softmax по горизонтали.\n$V$: $(\\textrm{seq\\_len}, d_v)$ - матрица значений (аналогия: семантическая информация слова, которое суммируется со скорами). Полученную на предыдущем шаге матрицу домножаем на $V$, получаем взвешенное значение векторов из $V$ для каждого запроса\n$$X_{pos} \\cdot W^Q = Q, \\ X_{pos} \\cdot W^K = K, \\ X_{pos} \\cdot W^V = V; \\ \\textrm{Output} = \\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V: (\\textrm{seq\\_len}, d_v)$$\n**MuliHead attention**: получаем $h$ результатов на основе $3h$ обучаемых матриц: $\\textrm{head}_i$. Также есть ещё одна матрица $W_O$: (d_model, d_model), d_model = $h \\cdot d_v$\n$$\\textrm{Output} = \\textrm{Concat}(\\textrm{head}_1, ... \\textrm{head}_h) W^O: \\ (\\textrm{seq\\_len}, \\textrm{d\\_model}) \\cdot (\\textrm{d\\_model}, \\textrm{d\\_model}) = (\\textrm{d\\_model}, \\textrm{d\\_model})$$\n\n**Маски**: 1) предотвращение утечки будущего, 2) обработка паддинг-токенов\n$$ \\textrm{Output} = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}} + \\textrm{Mask}) V$$\nCross-attention для архитектуры энкодер-декодер: $Q$ из декодера, $K$ и $V$ из энкодера\n\n3\\. **Residual Connection + Layer Normalization**\n$$\\textrm{Output}_\\textrm{norm} = \\textrm{LayerNorm}(\\textrm{Output} + X_{pos})$$\n\n4\\. **Feed Forward** (e.g. d_model -> 4d_model -> d_model)\n$$\\textrm{FFN}(x) = \\textrm{ReLU}(x W_1 + b_1) W_2 + b_2$$\n И дальше снова Add + Normalize:\n \n $$X_\\textrm{fin} = \\textrm{LayerNorm} (X_\\textrm{new} + \\textrm{FFN}(X_\\textrm{new}))$$","x":860,"y":1340,"width":780,"height":1300},
		{"id":"744b31c038bb6298","type":"text","text":"### L5 Reinforcement learning\n\n1\\. **Basics**\nЧто у нас есть:\n- Объекты, обычно без ответов\n- Лосс, недифференцируемый, часто сложно сформулировать\n- Семейство моделей $f \\in \\mathcal{F}$, $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$\nЗадача: найти оптимальное отображение $f^* = \\textrm{argmin}_f L(f(x), y)$\n\nEnvironment $\\xrightarrow{\\textrm{observation}}$ agent, policy $\\pi$  $\\xrightarrow{\\textrm{action}}$ feedback\n\n- State $s \\in S$\n- Action $a \\in A$\n- Reward $r \\in \\mathbb{R}$ \n$$R = \\sum_{i=1}^\\infty r_i, \\ R_\\gamma = \\sum_{i=1}^\\infty \\gamma^i r_i \\ (\\gamma^i \\in (0,1))$$\nAssuming Markov property:\n$$P(S_{t+1}|S_t, a_t, ..., S_0, a_0 ) = P(S_{t+1}|S_t, a_t)$$\n\nOptimal reward: a) next step, b) long term\nExploration/exploitation tradeoff\n\n2\\. **Cross-entropy method**\n\nТабличный случай:\n1) Инициализируем стратегию (state-action matrix, каждая строка суммируется к 1)\n2) Сэмплируем $N$ сессий\n3) Выбираем $M$ элитных сессий с наибольшими ревардами\n4) Обновляем полиси, используя элитные сессии\n5) Повторяем\n\n$$\\pi(a|s) = A_{s,a}, \\ \\textrm{Elite} = [(s_0, a_0), (s_1, a_1), ..., (s_N, a_N)]$$\n$$\\pi_{new}(a|s) = \\frac{\\sum_{s_t, a_t \\in \\textrm{Elite}}[s_t = s][a_t = a]}{\\sum_{s_t, a_t \\in \\textrm{Elite}} [s_t = s]}$$\n\nБолее сложный случай. Пример: машинка на прямой, которая может ехать влево/вправо с разной интенсивностью ($s \\in \\mathbb{R}$, $a \\in \\mathbb{R}$)\n$$\\pi_\\theta(a|s) = f_\\theta(s, a) \\textrm{ - дифференцируемая по } \\theta$$\n$s, \\theta$ -> Linear(2, 100) -> vector (100, 1) -> Linear(100, 2)+SM -> probs (налево/направо + интенсивность)\nПриближённый метод (число состояний бесконечно):\n1) Модель (например, параметрическая) предсказывает вероятность действия в данном состоянии:  $$\\pi(a|s) = f_\\theta(a, s)$$\n2) Сэмплируем $N$ сессий, выбираем $M$ элитных\n3) Максимизируем likelihood действий в элитных сессиях $$\\pi(a|s)_{new} = \\textrm{argmax}_\\pi \\sum_{s_t, a_t \\in \\textrm{Elite}} \\log \\pi(a_i|s_i)$$\nЕсли множество действий непрерывно, то модель сэмплирует действия из некоторого подходящего распределения $$\\pi(a|s) = \\mathcal{N} (\\mu_\\theta (a, s), \\sigma_\\gamma(a, s)) $$\n$\\mu_\\theta$ - одна модель, предсказывает оптимальное действие в данном состоянии\n$\\sigma_\\gamma$ - моделирует конфиденс первой модели\n\n- model = RandomForestRegressor() - предсказывает действие по стейту\n- N сессий, M элитных сессий\n- Максимизируем правдоподобие в элитных сессиях: model.fit(elite_states, elite_actions)","x":-420,"y":2640,"width":740,"height":1640},
		{"id":"ed80d39085f6312a","type":"text","text":"### L6 Model-based RL\n\n1\\. **Введение**\nCumulative reward is called a return:\n$$G_t := R_t + R_{t+1} + ... + R_T$$\nExamples: \n1) data center non-stop cooling system; if R = 0 or 1, then inifinite reward for non-optimal behaviour\n2) moving to destination; $R = \\max(0, d(x, B) - d(x', B))$, then circle walk hacks the reward\n\nReward discounting: $$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\nEnd-of-effect process (?):\n$$G_0 = R_0 + \\gamma R_1 + \\gamma^2 R_2 + ... + \\gamma^T R_T = $$$$= (1-\\gamma)R_0 + (1-\\gamma)\\gamma(R_0 + R_1) + (1-\\gamma)\\gamma^2(R_0 + R_1 + R_2) ... + \\gamma^T \\sum_{t=0}^TR_t$$\nС вероятностью $(1-\\gamma)$ эффект заканчивается в конкретный момент, с вероятностью $\\gamma$ переносится дальше\n\nСдвиг реворда - плохая идея (если есть цикл, то можем сделать его с положительной сумой)\nМожем делать scaling, reward shaping (функция-потенциал)\n$$F(s, a, a') = \\gamma \\Phi(s')-\\Phi(s)$$\n\n2\\. **Оптимальная стратегия с максимизацией ожидаемого return**\n\n$$E[G_0] = E_{s_0}[E_{a_0|s_0}[R_0 + E_{s_1|s_0, a_0}[E_{a_1|s_1}[\\gamma R_1 + ...]]]]$$ **State-value function** $v(s)$ - expected return conditional on state:\n(о среде всё известно => есть распределение вероятности на реворды и на следующие стейты при условии действия $a$ в состоянии $s$ - $p(r,s'|s,a)$)\n$$v_\\pi(s) = E_\\pi [G_t |S_t = s] = E_\\pi[R_t + \\gamma G_{t+1}|S_t = S] =$$\n$$\\sum_a \\pi (a|s) \\sum_{r, s'} p(r, s'|s, a) [r + \\gamma E_\\pi [G_{t+1}|S_{t+1} = s']] = \\sum_a \\pi(a|s) \\sum_{r, s'} p(r,s'|s, a) [r + \\gamma v_\\pi(s')]$$\n\n**Action-value function** $q(s,a)$\n$$q_\\pi(s,a) = E_\\pi [G_t |S_t = s, A_t = 0] = E_\\pi[R_t + \\gamma G_{t+1}|S_t = S, A_t = a] =$$\n$$\\sum_{r, s'} p(r, s'|s, a) [r + \\gamma E_\\pi [G_{t+1}|S_{t+1} = s']] = \\sum_{r, s'} p(r,s'|s, a) [r + \\gamma v_\\pi(s')]$$\n\nRelations between them:\n$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{r, s'} p(r,s'|s,a) [r + \\gamma v_\\pi(s')] = \\sum_a \\pi(a|s) q_\\pi(s,a) $$\n$$q_\\pi(s, a) = \\sum_{r,s'}p(r,s'|s,a) [r+ \\gamma \\sum_{a'} \\pi(a'|s')q_\\pi(s',a')]$$\nДалее задача: оценить $q$, $v$, имея среду\n\n3\\. **Bellman optimality equations**\n$$\\pi \\geq \\pi' \\Leftrightarrow v_\\pi(s) \\geq v_{\\pi'}(s) \\ \\forall s$$\nBest policy $\\pi_*$ is better or equal to any other policy\n$v_{*}(s) = \\max_\\pi v_\\pi(s), \\ q_{*}(s) = \\max_\\pi q_\\pi(s,a)$ \n\nBellman optimality equation for $v(s)$:\n$$v_*(s) = \\max_a \\sum_{r,s'}p(r,s'|s,a) [r + \\gamma v_*(s')] = \\max_a E[R_t + \\gamma v_* (S_{t+1}) |S_t = s, A_t = a]$$\n\n4\\. **Policy iteration**\n\n1) policy evaluation $$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{r, s'} p(r,s'|s, a) [r + \\gamma v_\\pi(s')] \\textrm{ - system of linear equations}$$\n2) policy improvement\n$$\\pi'(s) \\leftarrow \\arg \\max_a \\sum_{r,s'} p(r,s'|s,a) [r + \\gamma v_\\pi(s')] = q_\\pi(s,a)$$\nгарантированно даёт не худшую стратегию \n\nПроцесс: $\\pi_0$ -> Policty Evaluation -> $v_{\\pi_0}(s) \\ \\forall s$ -> Policy Improvement -> $\\pi_1$ \n\n5\\. **Value iteration**\nСтратегия поддерживается неявно: $\\pi(s)$ - argmax $q(s,a)$\nОдин этап policy improvement:\n$$V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r|s,a)[r + \\gamma V(s')]$$","x":320,"y":2640,"width":720,"height":2000},
		{"id":"68c6454968d102d4","type":"text","text":"### L7 Model-free Rl\n\n1\\. **Monte-Carlo Policy Evaluation**\n1. MDP inner structure is unknown\n2. Can only try stuff and estimate\n3. Need samples of past experiences to learn\n\nModel-free: learning from trajectories\n- Sample a lot of sessions from our current pi\n- Look at the cumulative returns for each state\n- Average every visit\n\nMonte-Carlo Policy Evaluation\nТрекаем, сколько были в каждом состоянии и сколько реворда получили в данном состоянии\n$N(s) \\leftarrow N(s) + 1$, $S(s) \\leftarrow S(s) + G_t$, $V(s) = S(s) / N(s)$\n$V(s) \\rightarrow v_\\pi(s)$ as $N \\rightarrow \\infty$\n\nIncremental MC Policy Evaluation \nRunning mean updates:\n$$\\mu_k = \\frac{1}{k}\\sum_{j=1}^k x_j = \\mu_{k-1} + \\frac{1}{k} (x_k - \\mu_{k-1})$$\n$$N(S_t) \\leftarrow N(S_t) + 1, V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t)$$\n$G_t$ - значение реворда из нового сэмпла\nМожно константный LR ($\\alpha$ вместо $1/N(S_t)$)\n\nПроблема: апдейты редки для больших игр\nИдея: делать один шаг (Temporal Difference backup)\n$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$\n\nDynamic Programming backup - смотрим на все траектории, делаем апдейт, смотря на один шаг\n\nTD vs. MC\nMonte-Carlo:\n- honest sampling\n- unbiasedly estimates expectation\n- converges to MSE solution with $G_t$ targets\nTD: \n- bootstrapping\n- biased estimator (as 'target' involves error)\n- converges to solution of max likelihood Markov model\n\nTradeoff: TD($n$)\n\n2\\. **SARSA**\nEpsilon-greedy exploration:\n- with probability epsilon, pick random action uniformly\n- with probability (1 - eps), pick current best chozzice\n$$\\pi(a|s) = \\begin{cases}  \\epsilon /m + 1 - \\epsilon , \\ a^* = \\arg \\max_{a \\in \\mathcal{A}} Q(s, a) \\\\ \\epsilon/m, \\ \\textrm{otherwise} \\end{cases}$$\n\nModel Free Control: SARSA\n- One-step Temporal Difference Policy Evaluation\n- Epsilon-greedy Policy Improvement'\nS -> A, R -> S' -> A' (для S' знаем действие из стратегии)\n$Q(S, A) \\leftarrow Q(S,A) + \\alpha(R + \\gamma Q(S',A') - Q(S,A))$\nМожно n-step SARSA \n\nOff-policy","x":1040,"y":2640,"width":640,"height":1620},
		{"id":"bb984cf54bee5062","type":"text","text":"### S5\nExample. $r_p > R_0, \\ r_p < R_1, \\ r_p < R_2, ...$ \n$[s_0, ..., s_M], \\ [a_0, ..., a_M]$ => $\\pi$ \n```python\nn_states = 500\nn_actions = 6\n\npolicy = np.full((n_states, n_actions), 1./n_actions) # random policy at start\n\ndef generate_session(policy, t_max = int(10**4)):\n\t...\n\ta = np.random.choice(n_actions, p = policy[s])\n\tnew_s, r, done, info = env.step(a)\n\t\n\tstates.append(s)\n\tactions.append(a)\n\ttotal_reward += r\n\t\n\ts = s_new\n\tif done:\n\t\tbreak\n\t...\n\ndef select_elites(states_batch, actions_batch, rewards_batch, percentile = 50):\n\treward_threshold = np.percentile(rewards_batch, q = percentile)\n\telite_states, elite_actions = [], []\n\tfor state, action, reward in zip(states_batch, actions_batch, rewards_batch):\n\t\tif reward >= revward_threshold:\n\t\t\telite_states.append(states)\n\t\t\telite_actions.append(actions)\n\t\n\ndef update_policy(...):\n\tnew_policy = np.zeros([n_states, n_actions])\n\tfor state, action in zip(elite_states, elite_actions):\n\t\tnew_policy[state, action] += 1\n\tfor row in range(n_states):\n\tif np.sum(new_policy[row]) == 0:\n\t\tnew_policy[row] = np.ones(n_actions) / n_actions\n\telse:\n\t\tnew_policy[row] *= (1 / np.sum(new_policy[row]))\n\treturn new_policy\n```\n","x":-420,"y":4280,"width":740,"height":720},
		{"id":"24357523970ee0b9","type":"file","file":"2025-04-07_18-39-21.png","x":320,"y":4640,"width":528,"height":240},
		{"id":"6d56369b2fcb9be1","type":"text","text":"### L4 Transfer learning, CoVe, ELMO, BERT\n\n1\\. **Transfer learning**\nTL - техника использования свойств и распределения данных в на данной паре (задание, датасет) для других заданий и датасета\n\nTL: \n1) Transductive (tasks: same, labels: source task)\n\t -  Domain adaptation (different domains), e.g. translation -> translation with slang \n\t -  Cross-lingual learning (different languages)\n2) Inductive (tasks: different, labels: target tasks), e.g. word2vec\n\t - Multi-task learning (tasks learned simultaneously)\n\t - Sequential transfer learning (tasks learned sequentially)\n\n2\\. **CoVe, ELMO** \nCoVe\n- Train NMT encoder-decoder system\n- Take pre-trained encoder as feature extractor\n- CoVe = encoder outputs for a given sequence\nИз обученной на переводе rnn-ки извлекаем скрытые состояния, используем в такой же архитектуре\nELMO - то же самое, но для языкового моделирования\n\n3\\. **BERT**\nBidirectional Encoder Representation from Transformers\nIdea: language modelling task + transformer encoder\n\nЗадача: \n1) предсказание замаскированных токенов\n2) next sentence prediction\nCLS My dog is very cute SEP He likes playing in the garden with me SEP\n\nСтратегия для предсказания замаскированных токенов:\n1) рандомно выбираем 15% токенов\n2) данный токен а) заменён маской с 0.8, б) заменён рандомным словом с 0.1, в) не заменён с 0.1","x":2040,"y":-200,"width":740,"height":1020},
		{"id":"36230c987ed9221d","type":"file","file":"2025-04-05_10-59-28.png","x":1640,"y":207,"width":399,"height":203},
		{"id":"49546bfc8770a1d8","type":"file","file":"1.jpg","x":1640,"y":-200,"width":400,"height":407},
		{"id":"052932eac8a96ea0","type":"file","file":"2025-04-05_11-04-20.png","x":1640,"y":410,"width":399,"height":193},
		{"id":"3e3b82e530db8ee1","type":"text","text":"1\\. CoVE - на NMT, ELMo - на языковом моделировании \nСтатические эмбеддинги - одно фиксированное представление для слова\nКонтекстные эмбеддинги - зависят от окружения слова (омонимы)\n\nELMo - двунаправленный LSTM: прогоняем предложение, берём эмбеддинги\n\n2\\. BERT (Masked Language Model + Next Sentence Prediction)","x":2040,"y":820,"width":740,"height":340},
		{"id":"fffa21cba19fae5d","type":"text","text":"### Дополнительные материалы\n\n#### I Основы NLP\n\n1\\. Токенизация: **BPE**, **WordPiece**, **SentencePiece**\n- BPE (Byte Pair Encoding) - использует идею итеративно объединять часто встречающиеся последовательности символов (GPT)\n1. Начинает с алфавита (отдельные символы)\n2. Считает частоту всех пар символов в тексте\n3. Заменяет самую частую пару на новый токен\n4. Повторяет, пока не достигнет размера словаря\nE.g. \"low low low\" -> \"l o w l o w l o w\" ->\"lo w lo w lo w\"\n- WordPiece - улучшенная версия BPE (BERT)\n1. Базовый словарь (отдельные символы + частые слова)\n2. Обучает masked language model на корпусе\n3. Объединяет пары, которые максимизируют вероятность корректного предсказания текста в модели\n- SentencePiece - токенизация на уровне сырых байтов, без предварительной нормализации \n#### **Таблица сравнения**\n\n| **Модель**          | **Токенизатор**   | **Особенности**                                         | **Примеры слов**                    |\n| ------------------- | ----------------- | ------------------------------------------------------- | ----------------------------------- |\n| **GPT-3/4**         | **BPE**           | Базовый BPE с дообучением на данных OpenAI              | `\"ChatGPT\"` → `[\"Chat\", \"G\", \"PT\"]` |\n| **BERT**            | **WordPiece**     | Оптимизирован под MLM (предсказание масок)              | `\"unhappy\"` → `[\"un\", \"##happy\"]`   |\n| **T5**              | **SentencePiece** | Универсальный, работает с сырым текстом                 | `\"Hello!\"` → `[\"▁Hello\", \"!\"]`      |\n| **Llama 2/3**       | **BPE**           | Модифицированный BPE (больший словарь)                  | `\"github\"` → `[\"git\", \"hub\"]`       |\n| **RoBERTa**         | **BPE**           | BPE + агрессивное разбиение (больше подслов)            | `\"running\"` → `[\"run\", \"ning\"]`     |\n| **ELECTRA**         | **WordPiece**     | Как BERT, но для генеративно-дискриминативного обучения | `\"playing\"` → `[\"play\", \"##ing\"]`   |\n| **mT5 (мультияз.)** | **SentencePiece** | Поддержка 100+ языков без предобработки                 | `\"日本語\"` → `[\"日\", \"本\", \"語\"]`         |\n\n\n2\\. **FastText** - Facebook (2016) - для создания эмбеддингов слов, учитывающая морфологию\n1. Слово разбивается на подстроки (\"кот\" -> ко, от, кот)\n2. Эмбеддинг = сумма эмбеддингов n-грамм\nКорректно обрабатывает редкие и опечатанные слова, поддержка **OOV (out of vocabulary)**, эффективна для синтетических языков (агглютинативные языки - финский, турецкий, где слова хорошо образуются через суффиксы\n\ne.g. кот -> <к, ко, от, т> -> V(<к) + ... + V(т>) -> предсказываем контекст (пушистый, мяукает) -> лосс - кросс-энропия (skip-gram)\n\n3\\. Метрика **Perplexity** - оценивает, насколько хорошо языковая модель предсказывает текст. Чем ниже перплексия, тем лучше модель \"понимает\" данные.\n$$\\textrm{Perplexity} = \\exp (-\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_1, ..., w_{i-1}))$$","x":3080,"y":-200,"width":1080,"height":1240},
		{"id":"e1d5059e2b16cbbb","type":"text","text":"#### II LLM-модели\n| **Год**  | **Модель/Архитектура**    | **Прорыв**                                                                      | **Ограничения**                           |\n| -------- | ------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------- |\n| **2013** | Word2Vec (Mikolov)        | Первые эффективные _статические_ эмбеддинги слов (Skip-gram, CBOW).             | Нет контекста, только одиночные слова.    |\n| **2014** | Seq2Seq (Sutskever)       | Архитектура для перевода (encoder-decoder + RNN/LSTM).                          | Проблемы с длинными текстами (забывание). |\n| **2015** | Attention (Bahdanau)      | Механизм внимания для Seq2Seq (улучшил работу с контекстом).                    | Медленный из-за RNN.                      |\n| **2017** | **Transformer** (Vaswani) | Полностью внимательный (self-attention) подход. Основа всех современных LLM.    | Требует много данных и GPU.               |\n| **2018** | **GPT-1** (OpenAI)        | Первая _decoder-only_ модель на Transformer (предсказывает следующее слово).    | Маленький размер (117M параметров).       |\n| **2018** | **BERT** (Google)         | Первая _encoder-only_ модель (обучена на MLM + NSP). Поддержка контекста.       | Не генерирует текст, только анализирует.  |\n| **2019** | GPT-2                     | Увеличенный GPT (1.5B параметров) + zero-shot способности.                      | Риски misuse (фейковые тексты).           |\n| **2020** | **T5** (Google)           | Универсальная _encoder-decoder_ модель (всё — задача \"текст-в-текст\").          | Ресурсоёмкость.                           |\n| **2020** | GPT-3                     | 175B параметров, few-shot обучение. Шок возможностей.                           | Дорого, \"чёрный ящик\".                    |\n| **2021** | **Mixtral** (первые MoE)  | Смесь экспертов (_Mixture of Experts_): разные части модели — для разных задач. | Сложность обучения.                       |\n| **2022** | **ChatGPT** (GPT-3.5)     | Fine-tuning + RLHF (подстройка под диалог).                                     | Капризность, галлюцинации.                |\n| **2023** | **Llama 2** (Meta)        | Открытая модель (7B–70B), эффективная для локального использования.             | Слабее GPT-4 в сложных задачах.           |\n| **2024** | **GPT-4o** (OpenAI)       | Мультимодальность (текст, изображение, аудио), низкая задержка.                 | Закрытая архитектура.                     |\n\n##### Сравнение моделей семейства GPT (OpenAI)\n\n| **Модель**            | **Год** | **Параметры**         | **Объём данных**                          | **Архитектура**                     | **Ключевые особенности**                                  | **Ограничения**                                  |\n| --------------------- | ------- | --------------------- | ----------------------------------------- | ----------------------------------- | --------------------------------------------------------- | ------------------------------------------------ |\n| **GPT-1**             | 2018    | 117M                  | ~5GB (BooksCorpus)                        | Decoder-only Transformer (12 слоёв) | Первая LLM на Transformer, предсказывает следующее слово. | Маленький размер, слабые generative способности. |\n| **GPT-2**             | 2019    | 1.5B                  | 40GB (WebText)                            | Decoder-only (48 слоёв)             | Улучшенный масштаб, zero-shot выполнение задач.           | Риски генерации вредного контента.               |\n| **GPT-3**             | 2020    | 175B                  | 570GB (Common Crawl, книги, Wikipedia)    | Decoder-only (96 слоёв)             | Few-shot обучение, широкие возможности.                   | Дороговизна, энергозатратность.                  |\n| **GPT-3.5** (ChatGPT) | 2022    | 175B (дообученная)    | Данные + RLHF (человеческие предпочтения) | Decoder-only + RLHF                 | Оптимизация для диалога, меньше галлюцинаций.             | Капризность в сложных задачах.                   |\n| **GPT-4**             | 2023    | ~1.8T (оценка, MoE)   | Неизвестно (мультимодальные данные)       | Mixture of Experts (MoE)            | Мультимодальность (текст+изображение), высокая точность.  | Закрытая архитектура, дорогой API.               |\n| **GPT-4o**            | 2024    | Неизвестно (>= GPT-4) | Текст + аудио + изображения               | Улучшенный MoE                      | Режим real-time, низкая задержка.                         | Ограниченный доступ.                             |\n\n##### Сравнение моделей\n\n|**Модель**|**Тип**|**Параметры**|**Открытость**|**Лучшее применение**|\n|---|---|---|---|---|\n|**Mistral**|Decoder-only|7B–8x7B|Apache 2.0|Локальные чат-боты|\n|**Gemma**|Decoder-only|2B–7B|Проприетарная|Мобильные устройства|\n|**Falcon**|Decoder-only|7B–180B|Apache 2.0|Облачные решения|\n|**T5**|Encoder-Decoder|100M–11B|Apache 2.0|Текстовая переработка|\n|**Llama**|Decoder-only|7B–70B|Спецлицензия|Fine-tuning под свои нужды|\n\nT5 (Text-to-Text Transfer Transformer)**\n\n**Тип**: Encoder-Decoder (архитектура как у классических трансформеров для перевода)  \n**Разработчик**: Google (2020)  \n**Ключевая идея**: **Все задачи — текстовые**.\n**Особенности**:\n\n1. **Универсальность**:\n    \n    - Любая задача формулируется как \"текст → текст\":\n        \n        - Классификация: `\"Настроение: Я счастлив!\" → \"позитив\"`\n            \n        - Перевод: `\"Translate English to French: Hello → Bonjour\"`\n            \n        - Суммаризация: `\"Summarize: <длинный текст> → <короткий>\"`.\n            \n    - Единый подход вместо отдельных моделей для каждой задачи.\n        \n2. **Архитектура**:\n    \n    - **Encoder**: Анализирует входной текст (как BERT).\n        \n    - **Decoder**: Генерирует ответ (как GPT).\n        \n    - Между ними — **cross-attention** (связь энкодера и декодера).\n        \n3. **Обучение**:\n    \n    - Pretraining: Задача **«заполни пропуск»** (например, `\"Я люблю <mask> по утрам\" → \"кофе\"`).\n        \n    - Данные: **C4** (очищенный Common Crawl, 750 GB текста).\n        \n4. **Версии**:\n    \n    - `T5-small` (60M параметров) → `T5-11B` (11 млрд).\n        \n    - **mT5**: Мультиязычная версия (поддержка 101 языка).\n\n**Mixtral (Mixture of Experts, MoE)**\n\n**Тип**: Sparse Decoder-Only (разреженная архитектура)  \n**Разработчик**: Mistral AI (2023)  \n**Ключевая идея**: **Не все параметры активны сразу**.\n\n **Особенности**:\n\n1. **Архитектура MoE**:\n    \n    - Каждый слой содержит **8 «экспертов»** (подмоделей).\n        \n    - Для каждого токена **активны только 2 эксперта** (остальные «спят»).\n        \n    - Эффективный размер модели: **12.9B параметров** (но весов ~45B).\n        \n2. **Обучение**:\n    \n    - Данные: Аналогичны Llama 2 (открытые тексты + код).\n        \n    - Специфика: Эксперты учатся специализироваться на разных аспектах данных (например, код vs математика).\n        \n3. **Преимущества**:\n    \n    - **Скорость**: В 4–6x быстрее плотных моделей того же качества.\n        \n    - **Качество**: Сопоставимо с Llama 2 70B, но дешевле в инференсе.\n        \n    - **Длинный контекст**: Поддержка до 32K токенов.\n        \n4. **Пример работы**:\n    \n    - Запрос: `\"Напиши код на Python для QR-кода\"` → Активируются эксперты по **программированию**.\n        \n    - Запрос: `\"Объясни теорию относительности\"` → Активируются эксперты по **науке**.","x":4160,"y":-200,"width":1140,"height":2840},
		{"id":"1e553b33b1fba571","type":"text","text":"##### Пайплайн обучения (GPT)\n1\\. **Сбор данных** (максимально разнообразные и качественные)\n- **Текстовые корпуса**:\n    \n    - _Книги_ (Project Gutenberg) — 20–30% датасета.\n        \n    - _Научные статьи_ (arXiv, PubMed) — 10–15%.\n        \n    - _Веб-страницы_ (Common Crawl) — 40–50% (фильтрация через качественные домены).\n        \n    - _Код_ (GitHub) — 5–10% (для логических задач).\n        \n- **Мультимодальные данные** (GPT-4):\n    \n    - Изображения + текстовые описания (LAION-5B).\n        \n    - Аудио (подписи к подкастам).\n\n**Фильтрация**:\n\n- Удаление дубликатов (например, через MinHash).\n    \n- Очистка от вредного/конфиденциального контента (спам, личные данные).\n    \n- Классификация по языкам/тематикам.\n    \n\n**Объём**:\n\n- GPT-3: ~570 GB текста (300 млрд токенов).\n    \n- GPT-4: >1 трлн токенов + мультимодальные данные.\n---\n\n2\\. **Предобработка данных**  \n**Токенизация**:\n\n- Для GPT используется **BPE** (разбиение на субтокены).\n    \n- Размер словаря: 50K–100K токенов (например, `\"ChatGPT\"` → `[\"Chat\", \"G\", \"PT\"]`).\n    \n\n**Дополнительные шаги**:\n\n- Нормализация (приведение к нижнему регистру, удаление лишних пробелов).\n    \n- Фильтрация слишком коротких/длинных текстов.\n    \n- Балансировка по тематикам (чтобы модель не перекосило в сторону, например, новостей).\n\n3\\. **Обучение модели**\n**Архитектура**\n\n- **GPT**: Decoder-only Transformer (12–96 слоёв, 768–12288 размер эмбеддинга).\n    \n- **Обучение**: Авторегрессия (предсказание следующего слова).\n    \n\n **Этапы обучения**\n\n1. **Pretraining** (3–6 месяцев на тысячах GPU):\n    \n    - Задача: Предсказать следующее слово в последовательности.\n        \n    - Оптимизация: Cross-entropy loss.\n        \n    - Параметры:\n        \n        - Batch size: 3–10 млн токенов.\n            \n        - Learning rate: 1e-4 с постепенным снижением.\n            \n2. **Fine-tuning** (недели/месяцы):\n    \n    - **Этап 1**: Дообучение на узкоспециализированных данных (код, диалоги).\n        \n    - **Этап 2** (для ChatGPT): **RLHF** (обучение с подкреплением на человеческих предпочтениях).\n        \n\n**Пример для RLHF**:\n\n- Сбор данных: Люди ранжируют ответы модели (лучше/хуже).\n    \n- Обучение: Reward Model предсказывает \"качество\" ответа, затем GPT оптимизируется под него через PPO.\n\n4\\. **Оптимизация и инференс**\n\n**Оптимизации для продакшена**:\n\n- **Квантование**: Сведение весов с FP16 до INT8/INT4 (уменьшение размера модели в 2–4x).\n    \n- **Дистилляция**: GPT-4 → GPT-4-turbo (упрощение без потери качества).\n    \n- **MoE** (для GPT-4): Только часть параметров активно во время инференса.\n    \n **Инференс**:\n\n- **Поддержка длинного контекста**: KV-caching (кеширование прошлых токенов).\n    \n- **Скорость**: Пакетная обработка запросов (batching).\n\n **5. Мониторинг и доработки**\n\n- **Контроль качества**:\n    \n    - Тесты на **галлюцинации** (например, фактчекинг).\n        \n    - Фильтрация вредных ответов (Moderation API).\n        \n- **Обновления**:\n    \n    - Ежемесячные дообучения на новых данных (например, актуальные события).\n        \n\n---\n\n**Alignment (выравнивание, или «настройка под цели») в LLM** — это этап **после предобучения (pretraining)**, на котором модель адаптируют, чтобы её поведение соответствовало человеческим ценностям и задачам.\n(после pre-training, перед финальным инференсом)\n\n **Методы Alignment**\n\n1. **Supervised Fine-Tuning (SFT)**\n\n- **Что делают**: Дообучают модель на «хороших» примерах (например, диалогах с правильными ответами).\n    \n- **Пример**: ChatGPT сначала учится на диалогах из ChatGPT Demo.\n    \n\n2. **RLHF (Reinforcement Learning from Human Feedback)**\n\n- **Шаги**:\n    \n    1. **Сбор данных**: Люди ранжируют ответы модели (лучше/хуже).\n        \n    2. **Обучение Reward Model**: Нейросеть учится предсказывать, какой ответ лучше.\n        \n    3. **Оптимизация через PPO**: Модель подстраивается под оценки Reward Model.\n        \n- **Пример**: GPT-4 после RLHF меньше генерирует вредный контент.\n    \n\n3. **Прямые методы (DPO, Constitutional AI)**\n\n- **DPO**: Замена RLHF на оптимизацию через сравнение пар ответов.\n    \n- **Constitutional AI**: Модель сама оценивает свои ответы по заданным правилам.\n    \n","x":5300,"y":-200,"width":1260,"height":2540},
		{"id":"21f8cf5b31b029fb","type":"text","text":"#### Fine-tuning vs. Prompting\n1\\. **Full Fine-tuning** - дообучение всех параметров модели на новых данных\n- Берём предобученную модель\n- Используем датасет для конкретной задачи \n- Обновляем все веса\nПлюсы: максимальная адаптация, высокое качество, минусы: много ресурсов, риск забывания\n\n2\\. **LoRA (Low-Rank Adaptation)** - метод **параметрически-эффективного дообучения**, при котором\n- **Основные веса модели замораживаются** (остаются неизменными)\n- Вводятся **маленькие \"адаптеры\"** — низкоранговые матрицы, которые обучаются на новых данных\n- Обновляется **только 0.1–10% параметров** вместо 100%\nАдаптеры - пара низкоранговых матриц (rk = 8..64), которые:\n- встраиваются параллельно к исходным весам модели\n- обновляются в процессе fine-tuning\n- суммируются с основными весами во время инференса\n\n$$W' = W + BA$$\n$A$ проецирует входы в $r$-мерное пространство, $B$ проецирует обратно в исходное пространство\n$d \\times k \\rightarrow r \\times(d + k)$\nЭкономит 90% памяти, сохраняет исходные знания модели\n\nПараметры:\n-  Rank(r) - чем выше, тем лучше качество, но тем больше параметров (8, 16, 32)\n- Alpha ($\\alpha)$ - коэффициент масштабирования, контролирует силу адаптеров\n- Target Modules - к каким слоям добавлять адаптеры (q_proj, v_proj)\n\n3\\. **QLoRA**\n- Квантует основную модель в 4-bit\n- Использует адаптеры как в LoRA\n- Запуск 65B модели на 1 GPU\n$$W_{4bit} = quantize(W), \\ W' = W_{4bit} + BA$$\n\n4\\. **Prompt Engineering**\nНастройка поведения через входной текст без изменения весов\n- Few-shot \nTranslate English to French:\nsea → mer\nsky → ciel\ncat → \n- Chain-of-Though: Пошаговые рассуждения \nQuestion: Если 3x + 5 = 20, то x =?\nReasoning: Вычтем 5... (20-5=15)... x=5\n- In-context Learning - модель обучается на лету через контекст\n```python\nprompt = \"\"\"\n1. яблоко → fruit\n2. морковь → vegetable\n3. банан → \n\"\"\"\n```","x":7860,"y":-200,"width":1200,"height":1200},
		{"id":"210c86c63c368455","type":"text","text":"#### Оптимизация\n\n1\\. **KV-Caching** \nПроблема: при генерации каждого нового токена модель повторно обрабатывает все предыдущие - избыточные вычисленния\nРешение: \n- Кэшировать ключи (K) и значения (V) для прошлых токенов\n- На шаге $t$ вычисляем только $Q_t$ (запрос для нового токена), а $K$ и $V$ берём из кэша\nЭффект: ускорение инференса в несколько раз (e.g. GPT-3 с 2048-ток контекстом -> экономия $\\sim$ 90% вычислений)\n\n2\\. **Speculative Decoding** \nПроблема: Авторегрессия (последовательная генерация) - узкое место: каждый токенн требует полного прохода модели\nРешение:\n- Быстрая \"черновая\" модель предсказывает $N$ токенов вперёд\n- Основная модель проверяет их за 1 проход\n- Если токены совпали - принимаем, иначе - откатываемся и генерируем корректно\nЭффект: ускорение в 2-3x (для простых текстов), в GPT-4 используется схожая техника\n\n3\\. **Квантизация (Quantization)** \nПроблема: Большие модели (LLaMA 70B) не помещаются в память GPU\nКвантзация — это сжатие весов модели путём уменьшения их числовой точности (например, с 32-битных чисел до 4-битных). Это критически важно для запуска LLM на слабом железе (ноутбуки, мобильные устройства).\n**Зачем нужно?**\n\n- **Экономия памяти**:\n    \n    - LLaMA 70B в `float32` → 280 ГБ, а в `int4` → всего 35 ГБ.\n        \n- **Ускорение вычислений**: 4-битные операции работают быстрее на поддерживаемом железе.\n\n- **Снижение энергопотребления**: Меньше данных → меньше операций → меньше тепловыделение.\n\n|Тип|Битность|Точность|Пример использования|\n|---|---|---|---|\n|**Полная (FP32)**|32 бита|Исходная точность|Обучение модели|\n|**FP16/BF16**|16 бит|Умеренная|Инференс на GPU|\n|**INT8**|8 бит|Приемлемая|Мобильные устройства|\n|**INT4**|4 бит|Ограниченная|Запуск на CPU (llama.cpp)|\nМетоды:\n- Наивная квантизация (простое приведение весов к меньшему числу бит), с4льно теряется точность\n- GGUF (GPT-Generated Unified Format), используется в llama.cpp. Квантует в int4/int5 с сохранением масштаба (scale) и смещения (offset) для каждого блока\nВеса делятся на блоки (e.g. 64 в блоке), для каждого блока $s$, $W_{quant}$ значения $s$ сохраняются отдельно для деквантизации\n- AWQ (Activation-aware weight quantization) - квантует не все веса одинаково, а выбирает важные (те, что влияют на выход)\n- GPTQ (Post-Training Quantization): берётся небольшой калибровочный датасет, для каждого слоя вычисляется ошибка квантизации и корректируется\nПроблемы: потеря точности, проблемы на сложных задачах\n\n4\\. **Flash Attention**\nПроблема: Традиционный attention требует `O(N^2)` памяти и операций → тормозит на длинных контекстах\nFlash Attention — революционный алгоритм вычисления механизма внимания, разработанный специально для ускорения работы Transformer-моделей на GPU. \n\nОбычный attention - две проблемы:\n1. Высокая потребность в памяти (O(N^2)) для N токенов\n2. Неэффективное использования GPU (множество чтения/запись в медленную HBM память), низкая утилизация вычислительных ресурсов\n\nFlash Attention: \n- Разделение на блоки (Tiling): Разбиваем $Q, K, V$ на небольшие блоки, помещающиеся в быструю SRMA-память (64-256 элементов)\n- Перерасчёт softmax: вычисляем статистики поэтапно, избегаем необходимости хранить всю матрицу внимания\n - Оптимизированные операции: объединение операций для минимизации обращений к памяти, использование специальных инструкций GPU\n\n|Метод|Время (мс)|Память (GB)|Ускорение|\n|---|---|---|---|\n|Стандартный|1200|64|1x|\n|Flash Attention|250|12|4.8x|\n|Flash Attention 2|180|8|6.7x|\n\n#### Вычисления на GPU\n**Ключевые отличия от CPU**:\n\n| Характеристика | CPU              | GPU                            |\n| -------------- | ---------------- | ------------------------------ |\n| **Ядра**       | 4-32 сложных     | 1000-10000 простых             |\n| **Задачи**     | Последовательные | Параллельные                   |\n| **Память**     | Большой кэш      | Высокая пропускная способность |\nАрхитектура (NVIDIA)\n\n1. **SM (Streaming Multiprocessor)**:\n    \n    - Основная вычислительная единица\n        \n    - В A100 GPU — 108 SM\n        \n    - Каждый SM содержит:\n        \n        - **CUDA-ядра** (для INT32/FP32)\n            \n        - **Tensor-ядра** (для матричных операций)\n            \n        - **Warp-юниты** (группы из 32 потоков)\n            \n2. **Память**:\n    \n    - **HBM (High Bandwidth Memory)**: Основная медленная память (80GB/s)\n        \n    - **Shared Memory**: Быстрая память внутри SM (~20TB/s)\n        \n    - **Регистры**: Самая быстрая память (1 цикл доступа)\n **3. Как GPU ускоряет матричные операции**\n\nПример матричного умножения в Transformer: $Q K^T$\n**CPU**:\n- Вычисляет последовательно каждый элемент\n- 1 операция за такт на ядро\n\n**GPU**\n- Разбивает матрицы на блоки\n- Каждый SM вычисляет свой блок параллельно\n- A100 может выполнять **312 TFLOPS** (триллионов операций в секунду)\n\n #### **5. Пример: Ускорение Self-Attention**\n\nБез оптимизаций:\n\n1. Чтение Q,K,V из HBM (медленно)\n    \n2. Вычисление внимания (O(N²) операций)\n    \n3. Запись обратно в HBM\n    \n\nС Flash Attention:\n\n1. Разбиваем на блоки 64x64\n    \n2. Загружаем блок в Shared Memory\n    \n3. Вычисляем внутри SM\n    \n4. Результат аккумулируем в регистрах\n    \n\n**Ускорение**: До 8x для последовательностей 16K\n\n**6. Ключевые термины**\n\n- **Warp**: Группа из 32 потоков (базовая единица выполнения)\n    \n- **Tensor Core**: Спецблок для матричных операций\n    \n- **HBM**: High Bandwidth Memory (основная \"медленная\" память)\n    \n- **SM**: Streaming Multiprocessor (вычислительный кластер)\n    \n- **CUDA**: Язык программирования для GPU\n    \n**7. Почему GPU быстрее для LLM?**\n\n1. **Параллелизм**: 1000 ядер работают одновременно\n    \n2. **Оптимизированные операции**: Специальные инструкции для матриц\n    \n3. **Локализация данных**: Shared Memory/кеш уменьшают задержки\n    \n\nПример для GPT-3:\n\n- На CPU: ~10 сек/токен\n    \n- На A100 GPU: ~20 мс/токен (ускорение в 500x)\n    \n\n**8. Ограничения GPU**\n\n- **Память**: Даже A100 (80GB) не вмещает Llama 70B без квантзации\n    \n- **Задержки**: Ожидание данных из HBM (90% времени!)\n    \n- **Сложность программирования**: Требуется CUDA/специальные оптимизации\n\nБез оптимизаций:\n\n1. Чтение Q,K,V из HBM (медленно)\n    \n2. Вычисление внимания (O(N²) операций)\n    \n3. Запись обратно в HBM\n    \n\nС Flash Attention:\n\n1. Разбиваем на блоки 64x64\n    \n2. Загружаем блок в Shared Memory\n    \n3. Вычисляем внутри SM\n    \n4. Результат аккумулируем в регистрах\n    \n\n**Ускорение**: До 8x для последовательностей 16K","x":6740,"y":-220,"width":1300,"height":4300}
	],
	"edges":[]
}